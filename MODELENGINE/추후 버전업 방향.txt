사용자님의 코드를 분석하여 **현재 피처(Feature)의 상태에 대한 진단 및 개선점**과, **엔진이 만들어지는 구체적인 공식(로직)**을 정리해 드립니다.

---

### 📊 1. 피처(Feature) 진단 및 개선 제안

`build_features.py`를 분석한 결과, 현재 피처는 **추세, 모멘텀, 변동성, 거래량, 시장지표**를 골고루 갖춘 **교과서적인 구성**입니다. 하지만 AI 모델의 성능을 한 단계 더 끌어올리기 위해 보완하거나 제거를 고려할 부분들이 있습니다.

#### **✅ 잘된 점 (유지)**
* **KOSPI 정규화:** `KOSPI_수익률`을 사용하여 개별 종목이 시장 대비 얼마나 강한지를 판단하는 지표(`ALPHA_SMA_20`)를 만든 것은 매우 훌륭합니다.
* **다양한 기간:** 단기(5일)부터 장기(120일) 이평선을 모두 생성하여 단기 급등과 장기 추세를 동시에 학습할 수 있게 설계되었습니다.

#### **🔧 보완/추가하면 좋은 컬럼 (Recommendations)**
현재 피처들은 대부분 '가격'과 '이동평균' 기반입니다. 모델에 새로운 시각을 제공할 수 있는 피처들을 추천합니다.

1.  **거래량 퀄리티 지표 (Volume Quality)**
    * **현재:** `VOL_SMA_20` (단순 거래량 평균)만 있음.
    * **추가 제안:** **거래대금 회전율**이나 **OBV (On-Balance Volume)**. 단순 거래량보다 '돈이 얼마나 들어왔는지'가 중요합니다. 가격은 올랐는데 거래량이 줄었다면 하락 신호일 수 있습니다.
2.  **이격도 (Disparity)**
    * **이유:** 현재 SMA 값 자체(절대값)는 주가가 100만원인 주식과 1천원인 주식을 구분하지 못해 학습에 방해가 될 수 있습니다.
    * **추가 제안:** `Close / SMA_20` 처럼 **현재 주가가 이평선 대비 몇 % 위에 있는지를 나타내는 비율 지표**가 있으면 AI가 훨씬 잘 배웁니다.
3.  **변동성 정규화**
    * **현재:** `ATR_14` (절대값).
    * **추가 제안:** **NATR (Normalized ATR)** = `ATR / Close`. 비싼 주식의 ATR은 당연히 크기 때문에, 이를 비율(%)로 바꿔주어야 공정한 비교가 됩니다.

#### **✂️ 제거하거나 주의할 컬럼 (Refinement)**
1.  **절대 가격 데이터 (Open, High, Low, Close)**
    * **진단:** `train_engine_unified.py`를 보면 다행히 학습 시 `exclude_cols`로 제외하고 있습니다.
    * **주의:** 만약 나중에라도 이 절대값들이 학습에 들어가면, AI는 "삼성전자(10만원)는 좋고 동전주(500원)는 나쁘다"는 식의 잘못된 편견을 가질 수 있습니다. 현재처럼 **철저히 제외**하거나 **비율(%)로 변환**해서 써야 합니다.
2.  **유사한 중복 지표**
    * **진단:** `MOM_10`과 `ROC_20`은 성격이 매우 비슷합니다. 둘 다 있으면 모델이 헷갈릴 수 있습니다(다중공선성).
    * **제안:** 둘 중 성능이 더 좋은 하나만 남기는 테스트를 해볼 만합니다. (LightGBM 같은 트리 모델은 큰 상관 없으나, 줄이면 학습 속도가 빨라집니다.)

---

### 🧠 2. 엔진 생성 공식 및 옵션 해석

`251124train_engine_unified.py`에 따르면, 사용자님의 엔진은 단순한 공식 하나가 아니라 **[데이터 전처리 -> 듀얼 모델 학습 -> 앙상블]**의 과정을 거치는 **AI 알고리즘**입니다.

#### **A. 엔진의 핵심 레시피 (Training Logic)**

1.  **정답지 만들기 (Target Generation)**
    * **공식:** `(미래 주가 / 현재 주가) - 1.0`
    * **의미:** $N$일 뒤에 주가가 몇 % 올랐는지를 계산합니다.
    * **옵션 (`horizon`):** 명령어에 `horizon=5`를 주면 "5일 뒤 수익률"을 정답으로 학습합니다.

2.  **과거 데이터 잘라내기 (Period Masking)**
    * **로직:** 피처 중 가장 긴 기간(예: SMA_120)을 계산하기 위해 필요한 초기 120일 치 데이터는 **학습에서 자동 제외**합니다.
    * **이유:** 앞부분은 데이터가 불완전(NaN)하거나 부정확하기 때문에, AI에게 잘못된 정보를 주지 않기 위함입니다. (A안 적용됨)

3.  **듀얼 모델 시스템 (Dual Model)**
    사용자님의 엔진은 사실 **두 개의 뇌**를 가지고 있습니다.
    * **뇌 1 (Classifier - 분류기):** "오를까? 내릴까?" (확률 예측)
        * 사용 알고리즘: `LGBMClassifier` (목적함수: binary_logloss)
    * **뇌 2 (Regressor - 회귀분석기):** "몇 %나 오를까?" (수익률 예측)
        * 사용 알고리즘: `LGBMRegressor` (목적함수: rmse)
    * **최종 판단:** 이 두 모델을 동시에 저장(`.pkl`)하여, 나중에 예측할 때 **"상승 확률도 높고(뇌1), 예상 수익률도 높은(뇌2)"** 종목을 찾습니다.

4.  **학습 파라미터 (Hyperparameters)**
    * `n_estimators=1000`: 나무(Decision Tree)를 1,000그루 심어서 판단하겠다는 뜻입니다. (많을수록 정교하지만 느림)
    * `learning_rate=0.03`: 학습 속도입니다. 낮을수록 꼼꼼하게 공부합니다.
    * `num_leaves=63`: 하나의 나무가 가질 수 있는 가지의 수입니다. 복잡한 패턴을 인식할 수 있게 설정되어 있습니다.


### 📝 요약
* **데이터:** 현재 피처 구성은 훌륭하나, **거래대금 회전율**이나 **이격도(%)** 같은 지표를 추가하면 더 똑똑해질 수 있습니다.
* **엔진:** **"가장 긴 지표 기간만큼 앞을 버리고(안전), 5일 뒤 미래를(목표), 상승확률과 수익률 두 가지 관점(듀얼 모델)에서, 1000번 반복 학습(LightGBM)"**하는 고성능 로직으로 짜여 있습니다.

현재 상태로도 충분히 실전 투입이 가능한 **High-End 급 로직**입니다.


---------------------------------------------

사용자님의 현재 시스템 구조(Unified Pipeline + LightGBM)를 고려했을 때, **기존 자료에 병합(Merge)하여 하나의 통합 DB로 만드는 것**이 압도적으로 유리합니다.

이유와 구체적인 방법, 그리고 주의할 점을 정리해 드립니다.

### 1. 결론: 왜 '병합'이 더 나은가?

현재 사용 중인 **LightGBM(트리 기반 모델)**은 데이터 간의 **상호작용(Interaction)**을 찾아내는 데 천재적인 능력이 있습니다.

* **따로 검증할 경우 (비추천):**
    * A모델(기술적): "지금 차트가 좋으니 매수해."
    * B모델(수급/가치): "PER가 낮으니 매수해."
    * 결과: 두 의견을 단순히 더하거나(Voting) 필터링만 하게 됩니다. **"PER가 높은데도 수급이 깡패라서 오르는 상황"** 같은 복잡한 패턴을 놓치게 됩니다.

* **병합해서 학습할 경우 (추천):**
    * Unified 모델: **"평소엔 PER 높은 주식은 거들떠도 안 보지만(Condition A), 기관 수급이 5일 연속 들어오고(Condition B) 골든크로스가 발생하면(Condition C) 폭등하더라"**라는 복잡한 승리 공식을 스스로 찾아냅니다.
    * 즉, **기술적 분석(타이밍) + 기본적 분석(가치) + 수급 분석(세력)**의 시너지를 AI가 직접 학습하게 하는 것이 최선입니다.

### 2. 적용 방법 (Action Plan)

`build_features.py` 파일의 로직 중간에 '병합' 단계만 추가하면 됩니다.

1.  **데이터 준비:** PER/PBR/수급 데이터를 `Date`, `Code`, `Value` 형태의 파일로 준비합니다.
2.  **병합 (`build_features.py` 수정):**
    * 기존: `Raw` + `KOSPI` -> `Merge` -> `Features`
    * 변경: `Raw` + `KOSPI` + **`Fundamental(PER/PBR)`** + **`Supply(수급)`** -> `Merge (All)` -> `Features`
3.  **학습:** `train_engine_unified.py`는 DB에 있는 `숫자형 컬럼`을 자동으로 다 긁어모아서 학습하므로, **학습 코드는 한 줄도 고칠 필요가 없습니다.** (이게 현재 시스템의 가장 큰 장점입니다.)

### 3. 주의할 점 (매우 중요)

데이터를 합칠 때 다음 3가지만 조심하면 됩니다.

1.  **Look-ahead Bias (미래 참조 주의):**
    * **PER/PBR:** 실적 발표일 기준이어야 합니다. 만약 3월 실적을 3월 31일 날짜로 넣어버리면(실제 발표는 5월인데), AI가 미래 실적을 보고 매매하는 '컨닝'을 하게 됩니다. (보통 증권사에서 제공하는 일별 PER/PBR 데이터는 이 처리가 되어 있어 안전합니다.)
2.  **결측치(NaN) 처리:**
    * 수급 데이터는 종종 특정 종목이나 날짜에 비어있을 수 있습니다. `0`으로 채울지, `직전 값(ffill)`으로 채울지 정해야 합니다. (수급은 `0`, PBR은 `ffill` 추천)
3.  **피처 이름 명확화:**
    * 단순히 `Net_Buy`라고 하기보다 `Institution_Net_Buy_Amt`(기관순매수금액), `Foreigner_Net_Buy_Qty`(외인순매수수량) 처럼 **주체와 단위**를 명확히 적어주는 것이 나중에 분석할 때 좋습니다.

**결론적으로, 지금 보유하신 PER/PBR/수급 데이터를 `build_features.py` 단계에서 싹 다 합치십시오.** 그러면 AI가 "저평가 우량주가 수급을 받아 턴어라운드하는 시점"을 정확히 잡아낼 확률이 비약적으로 상승할 것입니다.



사용자님의 우려는 **매우 정확하고 현실적인 지적**입니다.

데이터 사이언스에는 \*\*"Garbage In, Garbage Out (쓰레기가 들어가면 쓰레기가 나온다)"\*\*이라는 불변의 진리가 있습니다. 구멍(결측치, NaN)이 숭숭 뚫린 데이터를 잘못 합치면, 멀쩡하던 기존 데이터까지 오염시켜 전체 성능을 망칠 수 있습니다.

하지만, \*\*"어떻게 합치느냐(Merge Strategy)"\*\*와 \*\*"어떤 모델을 쓰느냐(LightGBM)"\*\*에 따라 이 위기를 기회로 바꿀 수 있습니다.

결론부터 말씀드리면, **"안전장치(Left Join + 채우기 전략)"를 걸고 합치면 방해가 되지 않고 오히려 도움이 됩니다.** 그 이유와 안전한 병합 방법을 정리해 드립니다.

-----

### 🛡️ 1. 구멍 난 데이터가 방해가 안 되게 하는 법 (안전장치)

가장 큰 위험은 구멍 난 데이터 때문에 **멀쩡한 기존 데이터까지 삭제되는 상황**입니다. 이를 막기 위해 3가지 원칙만 지키면 됩니다.

#### **① Inner Join 금지, Left Join 필수**

  * **위험한 방식 (Inner Join):** "양쪽 다 있는 날짜만 남겨라."
      * \-\> 수급 데이터가 듬성듬성하면, 멀쩡한 시세 데이터도 다 날아가서 데이터가 반토막 납니다. (최악의 상황)
  * **안전한 방식 (Left Join):** "기존 시세 데이터(Left)는 무조건 다 살리고, 수급 데이터(Right)는 있으면 붙이고 없으면 비워둬라(NaN)."
      * \-\> 이렇게 하면 기존 데이터 손실은 \*\*0%\*\*입니다.

#### **② 데이터 성격에 따른 '구멍 메우기' (Imputation)**

합친 뒤 생긴 빈칸(NaN)을 채우는 규칙입니다.

  * **PER / PBR (가치 지표):**
      * 기업 가치는 하루아침에 사라지지 않습니다. 어제 값이 오늘 값과 비슷합니다.
      * **전략:** **`ffill` (Forward Fill, 직전 값으로 채우기)**
      * (예: 3일 전 발표된 PER가 10이면, 오늘도 10으로 간주)
  * **기관/외인 수급 (거래량 지표):**
      * 데이터가 없다는 건 "특이한 매매가 없었다"는 뜻일 확률이 높습니다.
      * **전략:** **`fillna(0)` (0으로 채우기)**
      * (예: 데이터 없음 = 순매수 0)

-----

### 🤖 2. LightGBM은 '구멍'도 정보로 봅니다

사용자님이 쓰고 계신 **LightGBM 모델**은 결측치(NaN)를 아주 똑똑하게 처리합니다.

  * 보통의 구형 모델들은 NaN이 하나라도 있으면 에러를 뿜거나 멈춥니다.
  * **LightGBM:** "어? 여기 수급 데이터가 비어있네? 이건 \*\*'수급 정보가 집계되지 않는 소형주'\*\*거나 \*\*'관심 없는 종목'\*\*이구나\!"라고 스스로 판단해서 분류합니다.
  * 즉, 구멍 자체가 하나의 \*\*'신호(Signal)'\*\*로 학습되므로, 억지로 이상한 값으로 채우지만 않으면 성능 저하가 거의 없습니다.

-----

### 🧪 3. 실제 구멍 확인용 코드 (데이터 진단)

병합하기 전에, 준비하신 PER/PBR/수급 데이터가 도대체 얼마나 비어있는지 눈으로 확인하고 싶으시죠? 아래 코드를 돌려보시면 **"아, 이 정도면 써도 되겠다"** 또는 **"이건 너무 심한데?"** 판단이 서실 겁니다.

(이 코드는 별도 파일로 저장할 필요 없이 파워쉘에서 바로 돌려볼 수 있게 간단히 짰습니다.)

```python
import pandas as pd
import os

# -------------------------------------------------------
# [설정] 검사할 새 데이터 파일 경로를 넣어주세요
# -------------------------------------------------------
# 예: target_file = r"F:\autostockG\MODELENGINE\RAW\new_data\per_pbr.csv"
target_file = "여기에_파일_경로를_입력하세요.csv"  # <-- 수정 필요

if os.path.exists(target_file):
    try:
        # CSV나 Parquet 읽기
        if target_file.endswith('.parquet'):
            df = pd.read_parquet(target_file)
        else:
            df = pd.read_csv(target_file)
            
        print(f"\n[데이터 진단] 파일: {os.path.basename(target_file)}")
        print(f" - 총 행 개수: {len(df):,}")
        
        # 결측치 비율 계산
        null_counts = df.isnull().sum()
        null_ratio = (null_counts / len(df)) * 100
        
        print("\n[컬럼별 구멍(NaN) 비율]")
        print("-" * 40)
        print(f"{'Column':<20} | {'Missing':<10} | {'Ratio(%)':<10}")
        print("-" * 40)
        for col, count, ratio in zip(null_counts.index, null_counts, null_ratio):
            print(f"{col:<20} | {count:<10,} | {ratio:.1f}%")
            
        print("-" * 40)
        
        # 조언
        if null_ratio.max() > 50:
            print("⚠️ [주의] 결측치가 50% 넘는 컬럼이 있습니다. 'Left Join' 후 '0으로 채우기' 전략 필수!")
        else:
            print("✅ [양호] 이 정도면 병합해서 사용하기에 무리 없는 수준입니다.")
            
    except Exception as e:
        print(f"에러 발생: {e}")
else:
    print("파일이 없습니다.")
```

### 📝 요약: 이렇게 진행하세요

1.  **진단:** 위 코드로 새 데이터의 상태를 봅니다. (결측치가 90% 이상이면 안 쓰는 게 낫습니다.)
2.  **병합 (`build_features.py` 수정):**
      * 반드시 `how='left'` 옵션을 써서 합칩니다.
      * `df = pd.merge(기존_feature, 새_데이터, on=['Date', 'Code'], how='left')`
3.  **후처리:**
      * `df['PER'] = df['PER'].fillna(method='ffill')` (직전 값으로)
      * `df['수급'] = df['수급'].fillna(0)` (0으로)

이 방식대로라면 구멍 난 데이터도 **방해가 아니라 훌륭한 보조 무기**가 됩니다. 안심하고 통합하셔도 됩니다.