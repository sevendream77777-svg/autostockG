# raw_patch_final.py (ìµœì¢… í†µí•© ì™„ì„±ë³¸)
# ì—­í• : RAW ë°ì´í„°ì˜ ìµœì‹  ë‚ ì§œ í™•ì¸, ë°ì´í„° ìˆ˜ì§‘, ì¤‘ë³µ ì œê±° ë° ë³‘í•©ì„ ìë™ ìˆ˜í–‰í•©ë‹ˆë‹¤.

import os
import sys
import time
from datetime import datetime, timedelta, date
from typing import Optional, Tuple, List, Dict, Any

import pandas as pd
import numpy as np

# --- ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ (í•„ìˆ˜) ---
# pip install yfinance requests pykrx pandas numpy ê°€ í•„ìš”í•©ë‹ˆë‹¤.
import requests
import yfinance as yf

try:
    from pykrx import stock as krx_stock
    HAS_KRX = True
except Exception:
    HAS_KRX = False

# ---------------------------------------------------------
# 1. ê²½ë¡œ ì„¤ì • ë° ê³µí†µ ë³€ìˆ˜
# ---------------------------------------------------------

BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# [ê²½ë¡œ ì„¤ì •] RAW íŒŒì¼ì„ stocks í´ë” ë‚´ì— ì €ì¥ (ìš”ì²­ì— ë”°ë¼ ìˆ˜ì •ë¨)
STOCKS_DIR = os.path.join(BASE_DIR, "stocks")
RAW_MAIN = os.path.join(STOCKS_DIR, "all_stocks_cumulative.parquet")

START_DATE = "2015-01-01"

# ---------------------------------------------------------
# 2. ë¡œê¹… í•¨ìˆ˜
# ---------------------------------------------------------

def log(msg: str):
    """ì½˜ì†” ë¡œê·¸ìš©"""
    print(msg, flush=True)

# ---------------------------------------------------------
# 3. ë°ì´í„° ìˆ˜ì§‘ ì—”ì§„ (Full Integration)
# ---------------------------------------------------------

def load_all_codes() -> List[str]:
    """KOSPI + KOSDAQ ì „ì²´ ì¢…ëª©ì½”ë“œ ìˆ˜ì§‘ (Naver + KRX)"""
    codes = set()
    urls = [
        "https://api.stock.naver.com/marketindex/marketStock/KOSPI",
        "https://api.stock.naver.com/marketindex/marketStock/KOSDAQ",
    ]
    for url in urls:
        try:
            r = requests.get(url, timeout=5)
            if r.status_code == 200:
                js = r.json()
                stocks = js.get("stocks", [])
                for s in stocks:
                    c = str(s.get("code", "")).strip()
                    if len(c) == 6 and c.isdigit():
                        codes.add(c)
        except Exception:
            pass

    if HAS_KRX:
        try:
            df_kospi = krx_stock.get_market_ticker_list(market="KOSPI")
            df_kosdaq = krx_stock.get_market_ticker_list(market="KOSDAQ")
            for c in list(df_kospi) + list(df_kosdaq):
                c = str(c).zfill(6)
                if len(c) == 6 and c.isdigit():
                    codes.add(c)
        except Exception:
            pass

    codes = sorted(list(codes))
    log(f"[INFO] ì „ì²´ ì¢…ëª©ì½”ë“œ ìˆ˜ì§‘ ì™„ë£Œ: {len(codes)}ê°œ")
    return codes

# ==========================================================
# 3-1. 3ë‹¨ê³„ Fallback ìˆ˜ì§‘ í•¨ìˆ˜ (fetch_from_...)
# ==========================================================

def fetch_from_yahoo(code: str, start: str, end: str) -> pd.DataFrame:
    """1ì°¨: Yahoo Finance ìˆ˜ì§‘"""
    for suffix in [".KS", ".KQ"]:
        ticker = f"{code}{suffix}"
        try:
            df = yf.download(ticker, start=start, end=end, progress=False)
            if df is not None and not df.empty:
                df = df.reset_index()
                df["Code"] = code
                df = df.rename(columns={
                    "Date": "Date", "Open": "Open", "High": "High", 
                    "Low": "Low", "Close": "Close", "Volume": "Volume"
                })
                df = df[["Date", "Code", "Open", "High", "Low", "Close", "Volume"]]
                df["Date"] = pd.to_datetime(df["Date"])
                log(f"  [YAHOO] {code} ì„±ê³µ.")
                return df
        except Exception:
            pass
    return pd.DataFrame()


def fetch_from_naver(code: str, start: str, end: str) -> pd.DataFrame:
    """2ì°¨: Naver ì°¨íŠ¸ API ìˆ˜ì§‘"""
    url = f"https://api.stock.naver.com/stock/{code}/chart"
    params = {"period": "DAY", "count": "4000"} 

    try:
        r = requests.get(url, params=params, timeout=10)
        if r.status_code != 200: return pd.DataFrame()
        js = r.json()
        rows = js.get("chart", {}).get("result", [])
        if not rows: return pd.DataFrame()

        data = []
        for d in rows:
            dt = d.get("date")
            if dt is None: continue
            if isinstance(dt, str) and len(dt) == 8 and dt.isdigit(): dt = f"{dt[0:4]}-{dt[4:6]}-{dt[6:8]}"
            data.append([dt, code, d.get("open", 0), d.get("high", 0), d.get("low", 0), d.get("close", 0), d.get("volume", 0)])

        df = pd.DataFrame(data, columns=["Date", "Code", "Open", "High", "Low", "Close", "Volume"])
        df["Date"] = pd.to_datetime(df["Date"])
        df = df[(df["Date"] >= pd.to_datetime(start)) & (df["Date"] <= pd.to_datetime(end))]

        if df.empty: return pd.DataFrame()
        log(f"  [NAVER] {code} ì„±ê³µ.")
        return df

    except Exception:
        return pd.DataFrame()


def fetch_from_krx(code: str, start: str, end: str) -> pd.DataFrame:
    """3ì°¨: KRX (pykrx) ìˆ˜ì§‘"""
    if not HAS_KRX: return pd.DataFrame()

    try:
        s = start.replace("-", "")
        e = end.replace("-", "")
        df = krx_stock.get_market_ohlcv_by_date(s, e, code)
        if df is None or df.empty: return pd.DataFrame()

        df = df.reset_index()
        df["Code"] = code
        df = df.rename(columns={"ë‚ ì§œ": "Date", "ì‹œê°€": "Open", "ê³ ê°€": "High", "ì €ê°€": "Low", "ì¢…ê°€": "Close", "ê±°ë˜ëŸ‰": "Volume"})
        df = df[["Date", "Code", "Open", "High", "Low", "Close", "Volume"]]
        df["Date"] = pd.to_datetime(df["Date"])

        log(f"  [KRX] {code} ì„±ê³µ.")
        return df

    except Exception:
        return pd.DataFrame()


def fetch_ohlcv_multi_source(code: str, start: str, end: str, 
                             fail_log: List[str], fallback_log: List[str], krx_log: List[str]) -> pd.DataFrame:
    """3ë‹¨ê³„ fallback í¬í•¨í•œ í†µí•© OHLCV ìˆ˜ì§‘"""

    # 1) Yahoo
    df = fetch_from_yahoo(code, start, end)
    if not df.empty: return df

    # 2) Naver
    fallback_log.append(code)
    df = fetch_from_naver(code, start, end)
    if not df.empty: return df

    # 3) KRX
    krx_log.append(code)
    df = fetch_from_krx(code, start, end)
    if not df.empty: return df

    log(f"  [FAIL] {code} 3ë‹¨ê³„ ëª¨ë‘ ì‹¤íŒ¨")
    fail_log.append(code)
    return pd.DataFrame()


# ==========================================================
# 4. ë°ì´í„° ì•ˆì •í™” ë¡œì§ (safe_raw_patch_v3.py í†µí•©)
# ==========================================================

def normalize_numeric_series(val):
    """ìˆ«ì ì»¬ëŸ¼ ì•ˆì •í™”"""
    if val is None: return pd.Series([pd.NA])
    if isinstance(val, pd.Series): return pd.to_numeric(val, errors="coerce")
    if isinstance(val, np.ndarray): val = val.flatten()
    if isinstance(val, (int, float, str)): val = [val]
    if isinstance(val, (list, tuple)): return pd.to_numeric(pd.Series(val), errors="coerce")
    return pd.to_numeric(pd.Series([val]), errors="coerce")


def fetch_single_day_multi(code: str, date_obj: date):
    """1ì¼ì¹˜ OHLCVë§Œ ìˆ˜ì§‘í•˜ì—¬ DataFrame ë°˜í™˜."""
    date_str = date_obj.strftime("%Y-%m-%d")
    start = date_str
    end = (date_obj + timedelta(days=1)).strftime("%Y-%m-%d")

    fail_log, fb_log, krx_log = [], [], []

    df_full = fetch_ohlcv_multi_source(code, start, end, fail_log, fb_log, krx_log)

    if df_full is None or df_full.empty: return None, "empty"
    
    df_full["Date"] = pd.to_datetime(df_full["Date"])
    df_day = df_full[df_full["Date"].dt.date == date_obj].copy()
    
    if df_day.empty: return None, "empty"

    for col in ["Open", "High", "Low", "Close", "Volume"]:
        df_day[col] = normalize_numeric_series(df_day[col])

    df_day["Code"] = df_day["Code"].astype(str).str.zfill(6)
    
    return df_day, "success"


# ==========================================================
# 5. ë©”ì¸ ìë™ ì—…ë°ì´íŠ¸ ë° ë³‘í•© í•¨ìˆ˜ (RAW_PATCH ìµœì¢… ë¡œì§)
# ==========================================================

def get_latest_raw_date(raw_path: str) -> Optional[date]:
    """ë©”ì¸ RAW íŒŒì¼ì—ì„œ ê°€ì¥ ìµœì‹  ë‚ ì§œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if not os.path.exists(raw_path):
        return None
    try:
        df = pd.read_parquet(raw_path, columns=["Date"])
        df["Date"] = pd.to_datetime(df["Date"])
        return df["Date"].max().date()
    except Exception as e:
        log(f"[ERROR] RAW íŒŒì¼({raw_path}) ì½ê¸° ì‹¤íŒ¨: {e}")
        return None


# raw_patch_final.py íŒŒì¼ì˜ 5ë²ˆ ì„¹ì…˜ (auto_update_raw í•¨ìˆ˜)

def auto_update_raw():
    log("===== RAW_PATCH.PY: ìë™ ì—…ë°ì´íŠ¸ ë° ë³‘í•© ì‹œì‘ =====")
    
    # [í´ë” ìƒì„±] stocks í´ë”ê°€ ì—†ìœ¼ë©´ ë§Œë“­ë‹ˆë‹¤.
    os.makedirs(STOCKS_DIR, exist_ok=True)
    
    # 1. RAW íŒŒì¼ ì¡´ì¬ í™•ì¸ ë° ìµœì‹  ë‚ ì§œ í™•ì¸
    if not os.path.exists(RAW_MAIN):
        log(f"[WARN] ë©”ì¸ RAW íŒŒì¼ ì—†ìŒ ({RAW_MAIN}). ì „ì²´ RAW êµ¬ì¶•ì´ ë¨¼ì € í•„ìš”í•©ë‹ˆë‹¤.")
        return

    latest_date = get_latest_raw_date(RAW_MAIN)
    if latest_date is None:
        log("[FATAL] RAW íŒŒì¼ì´ ë¹„ì—ˆê±°ë‚˜ ë‚ ì§œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‘ì—… ì¢…ë£Œ.")
        return
        
    log(f"[INFO] í˜„ì¬ RAW ìµœì‹  ë‚ ì§œ: {latest_date}")

    # 2. ìˆ˜ì§‘í•  ë‚ ì§œ ëª©ë¡ ìƒì„± (ìµœì‹  ë‚ ì§œì˜ ë‹¤ìŒ ë‚ ë¶€í„° ì˜¤ëŠ˜ê¹Œì§€)
    start_date_to_fetch = latest_date + timedelta(days=1)
    today = datetime.now().date()
    
    fetch_dates = []
    current_date = start_date_to_fetch
    while current_date < today:
        
        # â¬‡ï¸â¬‡ï¸ [ì£¼ë§ ê±´ë„ˆë›°ê¸° ë¡œì§ í†µí•©] â¬‡ï¸â¬‡ï¸
        if current_date.weekday() < 5: # ì›”(0) ~ ê¸ˆ(4)ë§Œ ìˆ˜ì§‘ ëŒ€ìƒì— í¬í•¨
            fetch_dates.append(current_date)
        else:
            log(f"[SKIP] {current_date.strftime('%Y-%m-%d')} ì£¼ë§ì´ë¯€ë¡œ ê±´ë„ˆëœë‹ˆë‹¤.")
        # â¬†ï¸â¬†ï¸ [ì£¼ë§ ê±´ë„ˆë›°ê¸° ë¡œì§ í†µí•©] â¬†ï¸â¬†ï¸
        
        current_date += timedelta(days=1)

    if not fetch_dates:
        log("[INFO] ì—…ë°ì´íŠ¸í•  ìƒˆë¡œìš´ ë‚ ì§œê°€ ì—†ìŠµë‹ˆë‹¤. ì‘ì—… ì¢…ë£Œ.")
        return

    log(f"[INFO] ìˆ˜ì§‘í•  ë‚ ì§œ ë²”ìœ„: {fetch_dates[0]} ~ {fetch_dates[-1]} ({len(fetch_dates)}ì¼)")

    # 3. ë°ì´í„° ìˆ˜ì§‘ ë° ë³‘í•© ì¤€ë¹„ (ì´í•˜ ì½”ë“œ ë™ì¼)
    codes = load_all_codes()
    all_new_data = []
    
    for date_obj in fetch_dates:
        # ... (ì´í•˜ ìˆ˜ì§‘ ë¡œì§ì€ ë™ì¼) ...
        log(f"\n[FETCH] ë‚ ì§œ: {date_obj.strftime('%Y-%m-%d')} ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
        all_rows_for_day = []
        n_success = 0
        
        for code in codes:
            df_day, status = fetch_single_day_multi(code, date_obj)
            
            if status == "success" and df_day is not None and not df_day.empty:
                all_rows_for_day.append(df_day)
                n_success += 1
        
        if n_success > 0 and all_rows_for_day:
            full_day_df = pd.concat(all_rows_for_day, ignore_index=True)
            log(f"[SUCCESS] {date_obj.strftime('%Y-%m-%d')}: {n_success}ê°œ ì¢…ëª© ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µ.")
            all_new_data.append(full_day_df)
        elif n_success == 0:
            log(f"[INFO] {date_obj.strftime('%Y-%m-%d')}: ê±°ë˜ì¼ ì•„ë‹˜ ë˜ëŠ” ìˆ˜ì§‘ ì‹¤íŒ¨ë¡œ ê±´ë„ˆëœ€.")

    if not all_new_data:
        log("[INFO] ìˆ˜ì§‘ëœ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‘ì—… ì¢…ë£Œ.")
        return
        
    # 4. ê¸°ì¡´ RAW ë¡œë“œ ë° ìƒˆë¡œìš´ ë°ì´í„°ì™€ ë³‘í•©
    df_main = pd.read_parquet(RAW_MAIN)
    frames = [df_main] + all_new_data
    
    merged = pd.concat(frames, ignore_index=True)
    
    # 5. ì¤‘ë³µ ì œê±° ë° ìµœì¢… ì •ë¦¬
    merged["Date"] = pd.to_datetime(merged["Date"])
    merged["Code"] = merged["Code"].astype(str).str.zfill(6)
    merged = merged.drop_duplicates(subset=["Date", "Code"], keep='last')
    
    merged = merged.dropna(subset=["Date", "Code"])
    merged = merged.sort_values(["Date", "Code"]).reset_index(drop=True)

    # 6. ìµœì¢… ì €ì¥
    merged.to_parquet(RAW_MAIN)
    log(f"\nğŸ‰ [ì™„ë£Œ] RAW ìµœì¢… ì—…ë°ì´íŠ¸ ì™„ë£Œ.")
    log(f"       ê²½ë¡œ: {RAW_MAIN}")
    log(f"       ìµœì‹  ë‚ ì§œ: {merged['Date'].max().date()}, ì´ í–‰ìˆ˜: {len(merged):,}")
    log("===== RAW_PATCH.PY: ìë™ ì—…ë°ì´íŠ¸ ì™„ë£Œ =====")


if __name__ == "__main__":
    auto_update_raw()