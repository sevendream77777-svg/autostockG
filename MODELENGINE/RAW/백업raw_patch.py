# raw_patch.py  (V6 - Kiwoom 1st Source Added)
# ê¸°ì¡´ ë¡œì§ ì¼ì ˆ ë³€ê²½ ì—†ìŒ
# ì˜¤ì§ Kiwoom REST API ì „ì²´ ì¼ë´‰ ì¡°íšŒë¥¼ 1ìˆœìœ„ë¡œ ì¶”ê°€

import os
import sys
import time
import math
import datetime as dt
from functools import lru_cache
from typing import Optional, Tuple, List, Iterable, Callable

import pandas as pd
from pykrx import stock
import requests
import FinanceDataReader as fdr
import os as _os
import yfinance as yf
import os.path as _path

# ============================================================
#  KIWOOM REST API ê²½ë¡œ/ëª¨ë“ˆ ì„¤ì • (FM ë²„ì „, ì ˆëŒ€ ì˜¤ë™ì‘ ì—†ìŒ)
# ============================================================
# ìœ„ì¹˜: F:\autostockG\kiwoom_rest\  â† í˜¸ë´‰ì´ê°€ ìƒˆë¡œ ë§Œë“  REST ì „ìš© íŒ¨í‚¤ì§€
KIWOOM_REST_DIR = r"F:\autostockG"
if KIWOOM_REST_DIR not in sys.path:
    sys.path.append(KIWOOM_REST_DIR)

# REST API ì „ìš© ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸°
from kiwoom_rest.token_manager import KiwoomTokenManager
from kiwoom_rest.kiwoom_api import KiwoomRestApi

# ======================
# ê²½ë¡œ ì„¤ì •
# ======================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
STOCKS_DIR = os.path.join(BASE_DIR, "stocks")
RAW_MAIN = os.path.join(STOCKS_DIR, "all_stocks_cumulative.parquet")
DAILY_DIR = os.path.join(STOCKS_DIR, "DAILY")
LOG_DIR = os.path.join(STOCKS_DIR, "LOGS")
OHLCV_COLS = ["Open", "High", "Low", "Close", "Volume"]

os.makedirs(DAILY_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# MODELENGINE ë£¨íŠ¸ ê²½ë¡œë¥¼ sys.pathì— ì¶”ê°€í•˜ì—¬ UTIL ëª¨ë“ˆ ì‚¬ìš©
PROJECT_ROOT = os.path.abspath(os.path.join(BASE_DIR, ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

from UTIL.config_paths import versioned_filename

def print_header():
    print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ ğŸ‰ í°ë‘¥ì´ ì›ë³¸ë°ì´í„° ì—…ë°ì´íŠ¸ (V6)           â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print(f"[PATH] RAW_MAIN : {RAW_MAIN}")
    print()

def log(msg: str):
    ts = dt.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{ts}] {msg}")

def _invalid_ohlcv_mask(df: pd.DataFrame) -> pd.Series:
    subset = df[OHLCV_COLS] if all(col in df.columns for col in OHLCV_COLS) else df
    return subset.isna().any(axis=1) | (subset <= 0).any(axis=1)

# ======================
# ë‚ ì§œ ìœ í‹¸
# ======================
def to_ymd(d: dt.date) -> str:
    return d.strftime("%Y%m%d")

def parse_date(s: str) -> dt.date:
    s = str(s)
    if "-" in s:
        return dt.datetime.strptime(s, "%Y-%m-%d").date()
    return dt.datetime.strptime(s, "%Y%m%d").date()

@lru_cache(maxsize=512)
def _nearest_bizday_cached(date_ymd: str) -> str:
    return stock.get_nearest_business_day_in_a_week(date_ymd)

def is_trading_day(date: dt.date) -> bool:
    today = dt.date.today()
    if date == today:
        return date.weekday() < 5

    date_ymd = to_ymd(date)
    try:
        nearest = _nearest_bizday_cached(date_ymd)
        if nearest == date_ymd:
            return True
    except Exception as e:
        log(f"[WARN] pykrx ë‚ ì§œ í™•ì¸ ì‹¤íŒ¨ - {e}. ì£¼ë§ ì—¬ë¶€ë¡œë§Œ íŒì •.")

    # pykrx ì‹¤íŒ¨ ì‹œ FDR(ì½”ìŠ¤í”¼ ì§€ìˆ˜)ë¡œ íœ´ì¼ ì—¬ë¶€ ë³´ì¡° í™•ì¸
    try:
        df_tmp = fdr.DataReader("KS11", date, date)
        if df_tmp is not None and not df_tmp.empty:
            return True
    except Exception as e:
        log(f"[WARN] FDR íœ´ì¼ í™•ì¸ ì‹¤íŒ¨ - {e}. ìš”ì¼ë§Œ ì‚¬ìš©.")

    return date.weekday() < 5

def get_next_bizdate(last_date: dt.date) -> dt.date:
    d = last_date
    for _ in range(400):
        d = d + dt.timedelta(days=1)
        if is_trading_day(d):
            return d
    raise RuntimeError("ë‹¤ìŒ ì˜ì—…ì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

# ======================
# SAVE / LOAD
# ======================
def load_raw_main() -> pd.DataFrame:
    if not os.path.exists(RAW_MAIN):
        raise FileNotFoundError(f"RAW ë©”ì¸ íŒŒì¼ ì—†ìŒ: {RAW_MAIN}")
    df = pd.read_parquet(RAW_MAIN)
    df["Date"] = pd.to_datetime(df["Date"]).dt.date
    return df

def backup_raw_main(raw_df: pd.DataFrame, today: dt.date) -> str:
    backup_path = versioned_filename(RAW_MAIN)
    raw_df.to_parquet(backup_path)
    return backup_path

def merge_daily_into_raw(raw_df: pd.DataFrame, daily_df: pd.DataFrame) -> pd.DataFrame:
    merged = pd.concat([raw_df, daily_df], ignore_index=True)
    merged = merged.drop_duplicates(subset=["Date", "Code"], keep="last")
    merged = merged.sort_values(["Date", "Code"]).reset_index(drop=True)
    return merged

# =====================================================================================
# â­â­ 1ìˆœìœ„: Kiwoom REST API ì „ì²´ ì¼ë´‰ ì¡°íšŒ ì¶”ê°€ (í˜¸ì •ë‹˜ ìš”ì²­)
# =====================================================================================

def build_daily_from_kiwoom(date: dt.date, tickers: Optional[List[str]] = None) -> Tuple[pd.DataFrame, List[str]]:
    """
    ì „ì²´ ì¢…ëª© ì¼ë´‰ ì‹œì„¸ë¥¼ í‚¤ì›€ REST APIì—ì„œ ì¡°íšŒí•˜ì—¬ RAW í˜•ì‹ìœ¼ë¡œ ë°˜í™˜.
    (í‚¤ì›€ REST ì£¼ì‹ì¼ë´‰ì°¨íŠ¸ ka10081)
    """
    log(f"[STEP] KIWOOM ì „ì²´ ì¼ë´‰ ìˆ˜ì§‘ ì‹œì‘: {to_ymd(date)}")

    import configparser
    from kiwoom_rest.token_manager import KiwoomTokenManager

    cfg = configparser.ConfigParser()
    cfg.read(r"F:\autostockG\kiwoom_rest\config.ini")

    MODE = cfg["SETTINGS"]["MODE"].strip().lower()
    BASE_URL = cfg["SETTINGS"]["BASE_URL_PAPER"] if MODE == "paper" else cfg["SETTINGS"]["BASE_URL"]

    token_mgr = KiwoomTokenManager(
        config_file=r"F:\autostockG\kiwoom_rest\config.ini",
        token_file=r"F:\autostockG\kiwoom_rest\token.json"
    )
    token = token_mgr.get_access_token()

    # ì „ì²´ ì¢…ëª© ì½”ë“œ: pykrx ìš°ì„ , ì‹¤íŒ¨ ì‹œ RAW ì½”ë“œ ì‚¬ìš©
    if tickers is None:
        try:
            tickers = stock.get_market_ticker_list(date=to_ymd(date), market="ALL")
        except Exception:
            tickers = []
        if not tickers:
            try:
                tickers = load_raw_main()["Code"].unique().tolist()
            except Exception:
                tickers = []

    rows = []
    bad_codes = []
    target = to_ymd(date)
    timeout_sec = 2  # í•œ ì¢…ëª© ìš”ì²­ íƒ€ì„ì•„ì›ƒ(ì´ˆ)

    for idx, code in enumerate(tickers, 1):
        if idx % 100 == 0:
            log(f"[KIWOOM] ì§„í–‰ {idx}/{len(tickers)}")

        url = f"{BASE_URL}/api/dostk/chart"
        headers = {
            "Content-Type": "application/json;charset=UTF-8",
            "api-id": "ka10081",
            "authorization": f"Bearer {token}",
        }
        body = {
            "stk_cd": code,
            "base_dt": target,
            "upd_stkpc_tp": "0",  # ìˆ˜ì •ì£¼ê°€ êµ¬ë¶„: 0 ê¸°ë³¸, 1 ìˆ˜ì •
        }

        try:
            r = requests.post(url, headers=headers, json=body, timeout=timeout_sec)
            r.raise_for_status()
            js = r.json()

            chart = js.get("stk_dt_pole_chart_qry") or js.get("chart") or []
            if not chart:
                bad_codes.append(code)
                continue
            matched = None
            for item in chart:
                if str(item.get("dt")) == target:
                    matched = item
                    break
            if matched is None:
                matched = chart[0]

            def _to_float(v):
                try:
                    return float(str(v).replace("+", "").replace(",", "").strip())
                except Exception:
                    return float("nan")

            rows.append({
                "Date": date,
                "Open": _to_float(matched.get("open_pric")),
                "High": _to_float(matched.get("high_pric")),
                "Low": _to_float(matched.get("low_pric")),
                "Close": _to_float(matched.get("cur_prc")),
                "Volume": _to_float(matched.get("trde_qty")),
                "Change": 0.0,
                "Code": code,
                "Name": "",
                "Market": ""
            })

        except Exception:
            bad_codes.append(code)
            continue

    df = pd.DataFrame(rows)
    log(f"[KIWOOM] {len(df)}ê°œ ì¢…ëª© ìˆ˜ì§‘, ì‹¤íŒ¨ {len(bad_codes)}ê°œ")
    return df, bad_codes


# =====================================================================================
# â­ ê¸°ì¡´ KRX 2ìˆœìœ„ ê·¸ëŒ€ë¡œ ìœ ì§€
# =====================================================================================
def build_daily_from_pykrx(date: dt.date) -> Tuple[pd.DataFrame, List[str]]:
    # í”„ë¡ì‹œê°€ ë¡œì»¬ë¡œ ì„¤ì •ëœ í™˜ê²½ì„ ìš°íšŒ
    for key in ["http_proxy", "https_proxy", "HTTP_PROXY", "HTTPS_PROXY"]:
        _os.environ[key] = ""
    date_ymd = to_ymd(date)
    log(f"[STEP] KRX ì¼ê´„ ìˆ˜ì§‘ ì‹œì‘: {date_ymd}")

    df = stock.get_market_ohlcv_by_ticker(date_ymd, market="ALL")
    if df is None or df.empty:
        raise RuntimeError(f"KRX ë°ì´í„° ì—†ìŒ: {date_ymd}")

    # pykrxê°€ ì˜ì–´ ì»¬ëŸ¼ìœ¼ë¡œ ë‚´ë ¤ì£¼ëŠ” í™˜ê²½ ëŒ€ì‘
    eng_map = {"Open": "ì‹œê°€", "High": "ê³ ê°€", "Low": "ì €ê°€", "Close": "ì¢…ê°€", "Volume": "ê±°ë˜ëŸ‰"}
    if set(eng_map.keys()).issubset(df.columns):
        df = df.rename(columns=eng_map)

    rename_map = {
        "ì‹œê°€": "Open", "ê³ ê°€": "High", "ì €ê°€": "Low",
        "ì¢…ê°€": "Close", "ê±°ë˜ëŸ‰": "Volume", "ë“±ë½ë¥ ": "ChangePct"
    }
    df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})

    if "ChangePct" in df.columns:
        df["Change"] = df["ChangePct"] / 100.0
    else:
        df["Change"] = 0.0

    df = df.reset_index().rename(columns={"í‹°ì»¤": "Code"})

    def _get_name_safe(ticker: str) -> str:
        try:
            return stock.get_market_ticker_name(ticker)
        except:
            return ""

    df["Name"] = df["Code"].map(_get_name_safe)
    df["Date"] = date

    keep_cols = ["Date","Open","High","Low","Close","Volume","Change","Code","Name"]
    for col in keep_cols:
        if col not in df.columns:
            df[col] = pd.NA
    df = df[keep_cols]

    mask_bad = _invalid_ohlcv_mask(df)
    suspicious_codes = df.loc[mask_bad, "Code"].tolist()

    log(f"[KRX] {len(df)}ê°œ ì¢…ëª© ìˆ˜ì§‘, ì˜ì‹¬ {len(suspicious_codes)}ê°œ")
    return df, suspicious_codes


# =====================================================================================
# ê¸°ì¡´ ë„¤ì´ë²„ fallback ìœ ì§€
# =====================================================================================

def fetch_ohlcv_from_naver(ticker: str, yyyymmdd: str) -> Optional[dict]:
    url = f"https://api.finance.naver.com/siseJson.naver?symbol={ticker}&requestType=1&startTime={yyyymmdd}&endTime={yyyymmdd}"
    try:
        r = requests.get(url, timeout=5)
        arr = r.json()
        if not arr or len(arr) < 2:
            return None
        row = arr[1]
        return {
            "Open": float(row[1]), "High": float(row[2]),
            "Low": float(row[3]), "Close": float(row[4]),
            "Volume": float(row[5]),
        }
    except:
        return None

def fetch_ohlcv_from_fdr(ticker: str, yyyymmdd: str) -> Optional[dict]:
    try:
        start = dt.datetime.strptime(yyyymmdd, "%Y%m%d").date()
        end = start + dt.timedelta(days=1)
        df = fdr.DataReader(ticker, start, end)
        if df is None or df.empty:
            return None
        row = df.iloc[0]
        return {
            "Open": float(row["Open"]), "High": float(row["High"]),
            "Low": float(row["Low"]), "Close": float(row["Close"]),
            "Volume": float(row["Volume"]),
        }
    except:
        return None

def fetch_ohlcv_from_yahoo(ticker: str, yyyymmdd: str) -> Optional[dict]:
    try:
        dt_date = dt.datetime.strptime(yyyymmdd, "%Y%m%d").date()
        start = dt_date.strftime("%Y-%m-%d")
        end = (dt_date + dt.timedelta(days=1)).strftime("%Y-%m-%d")
        for suffix in [".KS", ".KQ", ""]:
            df = yf.download(f"{ticker}{suffix}", start=start, end=end, progress=False)
            if df is None or df.empty:
                continue
            row = df.iloc[0]
            return {
                "Open": float(row["Open"]), "High": float(row["High"]),
                "Low": float(row["Low"]), "Close": float(row["Close"]),
                "Volume": float(row["Volume"]),
            }
        return None
    except:
        return None

FALLBACK_SOURCES: Iterable[Tuple[str, Callable[[str,str], Optional[dict]]]] = [
    ("fdr", fetch_ohlcv_from_fdr),
    ("yahoo", fetch_ohlcv_from_yahoo),
    ("naver", fetch_ohlcv_from_naver),
]

def fill_missing_with_sources(daily_df: pd.DataFrame, date: dt.date, codes: List[str],
                              sources: Optional[Iterable[Tuple[str,Callable[[str,str],Optional[dict]]]]] = None):
    if not codes:
        return daily_df, []
    if sources is None:
        sources = FALLBACK_SOURCES

    date_ymd = to_ymd(date)
    unresolved = list(dict.fromkeys(codes))

    for source_name, fetcher in sources:
        if not unresolved:
            break
        log(f"[{source_name.upper()}] ë³´ì¡° ìˆ˜ì§‘ ì‹œì‘, {len(unresolved)}ê°œ ë‚¨ìŒ")

        still = []
        rows_update = {}

        for code in unresolved:
            o = fetcher(code, date_ymd)
            if not o:
                still.append(code)
                continue
            rows_update[code] = o

        for code, o in rows_update.items():
            idx = daily_df.index[daily_df["Code"] == code].tolist()
            if idx:
                i = idx[0]
                for col in ["Open","High","Low","Close","Volume"]:
                    daily_df.at[i, col] = o[col]
                daily_df.at[i, "Change"] = 0.0

        unresolved = still
        log(f"[{source_name.upper()}] ì²˜ë¦¬ í›„ ë‚¨ì€ ì½”ë“œ: {len(unresolved)}")

    return daily_df, unresolved


def build_daily_from_fallback_sources(date: dt.date, tickers: Iterable[str]) -> Tuple[pd.DataFrame, List[str]]:
    """
    pykrx/kiwoom ëª¨ë‘ ì‹¤íŒ¨í•  ë•Œ FDR/Yahoo/Naver ìˆœì„œë¡œ ì „ì²´ í‹°ì»¤ë¥¼ ìˆ˜ì§‘.
    """
    tickers = list(dict.fromkeys(tickers))
    base = pd.DataFrame({"Code": tickers})
    base["Date"] = date
    for col in ["Open","High","Low","Close","Volume","Change","Name"]:
        if col not in base.columns:
            base[col] = pd.NA

    daily_df, unresolved = fill_missing_with_sources(base, date, tickers, FALLBACK_SOURCES)
    mask_bad = _invalid_ohlcv_mask(daily_df)
    unresolved = list(dict.fromkeys(unresolved + daily_df.loc[mask_bad, "Code"].tolist()))
    log(f"[FALLBACK] ì„±ê³µ {len(daily_df) - len(unresolved)}ê±´, ì‹¤íŒ¨ {len(unresolved)}ê±´")
    return daily_df, unresolved


# =====================================================================================
# â­ ë©”ì¸ ì‹¤í–‰ë¶€
# =====================================================================================
if __name__ == "__main__":
    print_header()

    now_dt = dt.datetime.now()
    today = now_dt.date()
    raw_df = load_raw_main()

    last_date = raw_df["Date"].max()
    log(f"[STEP 1] RAW ìµœì‹  ë‚ ì§œ: {last_date}")

    # 16~18ì‹œëŠ” ì˜¤ì—¼ ë°©ì§€ë¡œ ì¤‘ë‹¨, 18ì‹œ ì´í›„ì—ëŠ” ë‹¹ì¼ ìˆ˜ì§‘ ì‹œë„
    if dt.time(16, 0) <= now_dt.time() < dt.time(18, 0):
        log("[WARN] 16~18ì‹œëŠ” ì˜¤ì—¼ ë°©ì§€ë¡œ ìˆ˜ì§‘ ì¤‘ë‹¨")
        sys.exit(0)

    try:
        now_t = now_dt.time()
        is_biz = is_trading_day(today)

        # 1) 16:00 ì´ì „ â†’ ë¬´ì¡°ê±´ ì „ì¼(last_date)
        if now_t < dt.time(16, 0):
            target_date = last_date

        # 2) 16:00~18:00 â†’ ê²½ê³  í›„ ì˜¤ëŠ˜ ìˆ˜ì§‘ í—ˆìš©(ì˜ì—…ì¼ì¼ ë•Œë§Œ)
        elif dt.time(16, 0) <= now_t < dt.time(18, 0):
            if is_biz:
                log("[WARN] 16~18ì‹œëŠ” ì˜¤ì—¼ ê°€ëŠ¥ì„±ì´ ìˆì–´ ì£¼ì˜ í•„ìš”")
                target_date = today
            else:
                target_date = last_date

        # 3) 18:00 ì´í›„ â†’ ì˜¤ëŠ˜ ì¥ ë§ˆê° ì´í›„ì´ë¯€ë¡œ today í—ˆìš©
        else:
            if is_biz:
                target_date = today
            else:
                target_date = last_date

        # 4) ë¯¸ë˜ ë‚ ì§œ ë°©ì§€ (ì˜¤ëŠ˜ë³´ë‹¤ í° ë‚ ì§œë©´ ê°•ì œë¡œ last_dateë¡œ ì¡°ì •)
        if target_date > today:
            log("[SAFEGUARD] ë¯¸ë˜ ë‚ ì§œë¡œ ì í”„ ì°¨ë‹¨ â†’ last_dateë¡œ ì¡°ì •")
            target_date = last_date

    except Exception as e:
        log(f"[ERROR] ë‚ ì§œ íŒì • ì‹¤íŒ¨: {e}")
        sys.exit(1)


    log(f"[STEP 2] ìˆ˜ì§‘ ê¸°ê°„: {target_date} ~ {target_date}")
    date = target_date

    # ===========================
    # â­â­ 1ìˆœìœ„: pykrx
    # ===========================
    try:
        daily_df, bad_codes = build_daily_from_pykrx(date)
        log("[OK] KRX(pykrx) ë°ì´í„° ì‚¬ìš©")
    except Exception as e:
        log(f"[WARN] KRX(pykrx) ì‹¤íŒ¨ â†’ {e}")
        daily_df = None
        bad_codes = []

    # ===========================
    # â­â­ 2ìˆœìœ„: KIWOOM
    # ===========================
    if daily_df is None or daily_df.empty:
        try:
            daily_df, bad_codes = build_daily_from_kiwoom(date)
            log("[OK] KIWOOM ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µ. 2ìˆœìœ„ ì†ŒìŠ¤ë¡œ ì‚¬ìš©.")
        except Exception as e:
            log(f"[WARN] KIWOOM ì‹¤íŒ¨ â†’ {e}")
            daily_df = None
            bad_codes = []

    # â­â­ 3ìˆœìœ„: FDR/Yahoo/Naver ì „ì²´ ìˆ˜ì§‘
    if daily_df is None or daily_df.empty:
        tickers = raw_df["Code"].unique().tolist()
        daily_df, bad_codes = build_daily_from_fallback_sources(date, tickers)
        if daily_df is None or daily_df.empty:
            log("[ERROR] FDR/Yahoo/Naver fallback ë„ ì‹¤íŒ¨")
            sys.exit(1)
        else:
            log("[OK] FDR/Yahoo/Naver fallback ì‚¬ìš©")

    # ===========================
    # â­â­ ë¶€ì¡±ë¶„ ë³´ì¡° ìˆ˜ì§‘: KIWOOM â†’ FDR/Yahoo/Naver
    # ===========================
    
    if bad_codes:
        # ì‚¬ìš©ì ì„ íƒì— ë”°ë¼ 'ì˜ì‹¬ ì½”ë“œ' ì¶”ê°€ ë°ì´í„° ê²€ì¦ ì—¬ë¶€ ê²°ì •
        try:
            ans = input(f"[QUERY] KRX ì˜ì‹¬ {len(bad_codes)}ê°œ ì¶”ê°€ ë°ì´í„° ê²€ì¦ì„ ì§„í–‰í• ê¹Œìš”? (y/n): ").strip().lower()
        except Exception:
            ans = "y"  # ë¹„ëŒ€í™”í˜• í™˜ê²½ ë³´í˜¸: ê¸°ë³¸ y

        if ans.startswith("y"):
            # (1) Kiwoomìœ¼ë¡œ ì˜ì‹¬ ì½”ë“œ ë³´ì™„ ì‹œë„
            try:
                kiw_df, _ = build_daily_from_kiwoom(date, tickers=bad_codes)
                if kiw_df is not None and not kiw_df.empty:
                    kiw_sub = kiw_df[kiw_df["Code"].isin(bad_codes)]
                    if not kiw_sub.empty:
                        daily_df = merge_daily_into_raw(daily_df, kiw_sub)
                        log(f"[KIWOOM] ë³´ì¡° ìˆ˜ì§‘ìœ¼ë¡œ {len(kiw_sub)}ê°œ ë®ì–´ì”€")
            except Exception as e:
                log(f"[WARN] KIWOOM ë³´ì¡° ìˆ˜ì§‘ ì‹¤íŒ¨ â†’ {e}")

            # (2) ë³´ì™„ í›„ ë‚¨ì€ ì˜ì‹¬ ì½”ë“œ ì•ˆë‚´ (FDR ê²€ì¦ ë‹¨ê³„ ì œê±°)
            mask_bad = _invalid_ohlcv_mask(daily_df)
            still_bad = daily_df.loc[mask_bad, "Code"].tolist()
            if still_bad:
                log(f"[INFO] ì¶”ê°€ ê²€ì¦ ì´í›„ ì—¬ì „íˆ ì˜ì‹¬/ê²°ì¸¡ ì¢…ëª© {len(still_bad)}ê°œ (ê²€ì¦ ë‹¨ê³„ ì¢…ë£Œ)")
        else:
            log("[SKIP] ì‚¬ìš©ì ì„ íƒì— ë”°ë¼ ì˜ì‹¬ ì½”ë“œ ì¶”ê°€ ê²€ì¦ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

    # ===========================
    # SAVE
    # ===========================
    out_path = os.path.join(DAILY_DIR, f"daily_{to_ymd(date)}.parquet")
    daily_df.to_parquet(out_path)
    log(f"[SAVE] DAILY ì €ì¥ ì™„ë£Œ: {out_path}")

    backup_path = backup_raw_main(raw_df, today)
    log(f"[BACKUP] RAW ë°±ì—… ìƒì„±: {backup_path}")

    merged = merge_daily_into_raw(raw_df, daily_df)
    merged.to_parquet(RAW_MAIN)
    log(f"[SAVE] RAW ë©”ì¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {RAW_MAIN}")

    log("[DONE] RAW ì—…ë°ì´íŠ¸ ë.")
