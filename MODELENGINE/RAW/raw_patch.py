# raw_patch.py  (V5)
# - KRX ì¼ê´„ ìˆ˜ì§‘(get_market_ohlcv_by_ticker)ë¡œ ì†ë„ ê°œì„ 
# - ë¶€ì¡±ë¶„ë§Œ Naver Finance(siseJson)ë¡œ ë°±ì—… ìˆ˜ì§‘
# - ì‹œê°„ëŒ€(16~20ì‹œ) ê²½ê³  + ì‚¬ìš©ìì˜ Yes/No ì¸í„°ë™ì…˜
# - RAW_MAIN ë°±ì—… ê·œì¹™: all_stocks_cumulative_YYMMDD[_n].parquet
# - DAILY/LOGS êµ¬ì¡° ìœ ì§€

import os
import sys
import time
import math
import datetime as dt
from functools import lru_cache
from typing import Optional, Tuple, List, Iterable, Callable

import pandas as pd
from pykrx import stock
import requests


# ===== ê²½ë¡œ ì„¤ì • =====
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
STOCKS_DIR = os.path.join(BASE_DIR, "stocks")
RAW_MAIN = os.path.join(STOCKS_DIR, "all_stocks_cumulative.parquet")
DAILY_DIR = os.path.join(STOCKS_DIR, "DAILY")
LOG_DIR = os.path.join(STOCKS_DIR, "LOGS")
OHLCV_COLS = ["Open", "High", "Low", "Close", "Volume"]

os.makedirs(DAILY_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)


# ===== ì½˜ì†” UI ìœ í‹¸ =====
def print_header():
    print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ ğŸ‰ í°ë‘¥ì´ì™€ í•¨ê»˜í•˜ëŠ” ì›ë³¸ë°ì´í„° ì—…ë°ì´íŠ¸ (V5)â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print(f"[PATH] RAW_MAIN : {RAW_MAIN}")
    print(f"[PATH] DAILY    : {DAILY_DIR}")
    print(f"[PATH] LOGS     : {LOG_DIR}")
    print()


def log(msg: str):
    ts = dt.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{ts}] {msg}")


def ask_yes_no(prompt: str) -> bool:
    while True:
        ans = input(f"{prompt} (y/n): ").strip().lower()
        if ans in ("y", "yes"):
            return True
        if ans in ("n", "no"):
            return False
        print("  â†’ y ë˜ëŠ” n ìœ¼ë¡œë§Œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")





def inline_progress(message: str):
    """í•œ ì¤„ì—ì„œ ì§„í–‰ ìƒí™©ì„ ê°±ì‹ í•œë‹¤."""
    sys.stdout.write("\r" + message)
    sys.stdout.flush()


def end_inline_progress():
    sys.stdout.write("\n")
    sys.stdout.flush()


def _invalid_ohlcv_mask(df: pd.DataFrame) -> pd.Series:
    """OHLCV ì¤‘ NaN/0/ìŒìˆ˜ë¥¼ í¬í•¨í•œ rowë¥¼ Trueë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    subset = df[OHLCV_COLS] if all(col in df.columns for col in OHLCV_COLS) else df
    return subset.isna().any(axis=1) | (subset <= 0).any(axis=1)


# ===== ë‚ ì§œ ê´€ë ¨ ìœ í‹¸ =====
def to_ymd(d: dt.date) -> str:
    return d.strftime("%Y%m%d")



def parse_date(s: str) -> dt.date:
    # s: "YYYY-MM-DD" ë˜ëŠ” "YYYYMMDD"
    s = str(s)
    if "-" in s:
        return dt.datetime.strptime(s, "%Y-%m-%d").date()
    return dt.datetime.strptime(s, "%Y%m%d").date()


@lru_cache(maxsize=512)
def _nearest_bizday_cached(date_ymd: str) -> str:
    """pykrx ê±°ë˜ì¼ API ê²°ê³¼ë¥¼ ìºì‹œí•©ë‹ˆë‹¤."""
    return stock.get_nearest_business_day_in_a_week(date_ymd)


def is_trading_day(date: dt.date) -> bool:
    """pykrx ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ ì˜ì—…ì¼ ì—¬ë¶€ë¥¼ íŒì •í•©ë‹ˆë‹¤."""
    today = dt.date.today()

    # ë‹¹ì¼ì€ ì¥ ì¢…ë£Œ ì „ê¹Œì§€ pykrxê°€ ì§ì „ ì˜ì—…ì¼ì„ ëŒë ¤ì£¼ë¯€ë¡œ ì£¼ë§ë§Œ ì œì™¸í•˜ê³  ì˜ì—…ì¼ë¡œ ë³¸ë‹¤
    if date == today:
        return date.weekday() < 5

    date_ymd = to_ymd(date)
    try:
        nearest = _nearest_bizday_cached(date_ymd)
    except Exception as e:
        log(f"[WARN] pykrx ë‚ ì§œ í™•ì¸ ì‹¤íŒ¨ - {e}. ì£¼ë§ ì—¬ë¶€ë§Œìœ¼ë¡œ íŒì •í•©ë‹ˆë‹¤.")
        return date.weekday() < 5
    return nearest == date_ymd


def get_next_bizdate(last_date: dt.date) -> dt.date:
    """ì£¼ë§/ê³µíœ´ì¼ì„ ëª¨ë‘ ê±´ë„ˆë›°ê³  pykrx ì˜ì—…ì¼ì„ ì°¾ìŠµë‹ˆë‹¤."""
    d = last_date
    for _ in range(400):
        d = d + dt.timedelta(days=1)
        if is_trading_day(d):
            return d
    raise RuntimeError("ë‹¤ìŒ ì˜ì—…ì¼ì„ 1ë…„ ì´ë‚´ì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")


def get_prev_bizdate(date: dt.date) -> dt.date:
    """ì£¼ì–´ì§„ ë‚ ì§œ ì´ì „ì˜ ê°€ì¥ ê°€ê¹Œìš´ ì˜ì—…ì¼."""
    d = date
    for _ in range(400):
        d = d - dt.timedelta(days=1)
        if is_trading_day(d):
            return d
    raise RuntimeError("ì´ì „ ì˜ì—…ì¼ì„ 1ë…„ ì´ë‚´ì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")


def fetch_ohlcv_from_naver(code: str, date_ymd: str) -> Optional[dict]:
    """
    Naver Finance API (ë¹„ê³µì‹)ì—ì„œ í•´ë‹¹ ì¢…ëª© í•˜ë£¨ OHLCV ê°€ì ¸ì˜¤ê¸°.
    https://api.finance.naver.com/siseJson.naver?symbol=005930&requestType=1&startTime=YYYYMMDD&endTime=YYYYMMDD&timeframe=day
    """
    url = (
        "https://api.finance.naver.com/siseJson.naver"
        f"?symbol={code}&requestType=1&startTime={date_ymd}&endTime={date_ymd}&timeframe=day"
    )
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Referer": f"https://finance.naver.com/item/sise_day.nhn?code={code}",
    }

    try:
        r = requests.get(url, headers=headers, timeout=5)
        r.raise_for_status()
        text = r.text.strip()

        # ì‘ë‹µ í¬ë§·ì´ JS ë°°ì—´ ë¬¸ìì—´ì´ë¼ ê°„ë‹¨íˆ íŒŒì‹±
        # ì˜ˆ: [[ë‚ ì§œ,ì‹œê°€,ê³ ê°€,ì €ê°€,ì¢…ê°€,ê±°ë˜ëŸ‰,ì™¸êµ­ì¸ì†Œì§„ìœ¨], [...], ...]
        if not text or "[" not in text:
            return None

        # ë§¨ ì•/ë’¤ ëŒ€ê´„í˜¸ ì œê±°
        # ì•ˆì „í•˜ê²Œ eval ëŒ€ì‹  pandas.read_json ë“±ì„ ì“°ê³  ì‹¶ì§€ë§Œ,
        # í•˜ë£¨ í•œ ì¤„ë§Œ í•„ìš”í•˜ë¯€ë¡œ ê°„ë‹¨ split ì‚¬ìš©
        rows = text.split("],[")
        if len(rows) <= 1:
            return None

        # ì²« ë²ˆì§¸ ì‹¤ì œ ë°ì´í„°ëŠ” rows[1] (rows[0]ì€ í—¤ë”)
        # ì˜ˆ: ["20190624", 45200, 45800, 45200, 45500, 6085066, 57.14
        data_part = rows[1]
        parts = data_part.replace("[", "").replace("]", "").split(",")

        date_str = parts[0].strip().replace('"', "").replace("'", "")
        open_p = float(parts[1])
        high_p = float(parts[2])
        low_p = float(parts[3])
        close_p = float(parts[4])
        volume = float(parts[5])

        return {
            "Date": dt.datetime.strptime(date_str, "%Y%m%d").date(),
            "Open": open_p,
            "High": high_p,
            "Low": low_p,
            "Close": close_p,
            "Volume": volume,
        }
    except Exception as e:
        log(f"[NAVER FAIL] {code} ({date_ymd}) - {e}")
        return None




FALLBACK_SOURCES: List[Tuple[str, Callable[[str, str], Optional[dict]]]] = [
    ("naver", fetch_ohlcv_from_naver),
]


# ===== RAW ë©”ì¸ ë¡œë”©/ë°±ì—…/ë³‘í•© =====
def load_raw_main() -> pd.DataFrame:
    if not os.path.exists(RAW_MAIN):
        raise FileNotFoundError(f"RAW ë©”ì¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {RAW_MAIN}")
    df = pd.read_parquet(RAW_MAIN)
    if "Date" not in df.columns:
        raise ValueError("RAW íŒŒì¼ì— 'Date' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    df["Date"] = pd.to_datetime(df["Date"]).dt.date
    return df



def backup_raw_main(raw_df: pd.DataFrame, today: dt.date) -> str:
    ymd_short = today.strftime("%y%m%d")  # 251119 í˜•íƒœ
    base_name = f"all_stocks_cumulative_{ymd_short}.parquet"
    backup_path = os.path.join(STOCKS_DIR, base_name)

    # ê²¹ì¹˜ë©´ _1, _2 ...
    idx = 1
    final_path = backup_path
    while os.path.exists(final_path):
        final_path = os.path.join(
            STOCKS_DIR,
            f"all_stocks_cumulative_{ymd_short}_{idx}.parquet",
        )
        idx += 1

    raw_df.to_parquet(final_path)
    return final_path


def merge_daily_into_raw(raw_df: pd.DataFrame, daily_df: pd.DataFrame) -> pd.DataFrame:
    # ë‹¨ìˆœ concat í›„ (Date, Code) ê¸°ì¤€ ì¤‘ë³µ ì œê±° + ì •ë ¬
    merged = pd.concat([raw_df, daily_df], ignore_index=True)
    merged = merged.drop_duplicates(subset=["Date", "Code"], keep="last")
    merged = merged.sort_values(["Date", "Code"]).reset_index(drop=True)
    return merged


# ===== ë©”ì¸ ë°ì´í„° ìˆ˜ì§‘ ë¡œì§ (V5 í•µì‹¬) =====
def build_daily_from_pykrx(date: dt.date) -> Tuple[pd.DataFrame, List[str]]:
    """
    1) pykrx.get_market_ohlcv_by_tickerë¡œ ì „ì²´ ì‹œì¥ í•˜ë£¨ OHLCV (ì´ˆê³ ì†)
    2) 'ë“±ë½ë¥ 'ì„ Change ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜ (ì†Œìˆ˜ë¡œ)
    3) Code/Name/Date ì»¬ëŸ¼ êµ¬ì„±
    4) OHLCV NaN/0 ì˜ì‹¬ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ë¦¬í„´ (ë°±ì—…ìš©)
    """
    date_ymd = to_ymd(date)
    log(f"[STEP] KRX ì¼ê´„ ìˆ˜ì§‘ ì‹œì‘: {date_ymd}")

    df = stock.get_market_ohlcv_by_ticker(date_ymd, market="ALL")
    if df is None or df.empty:
        raise RuntimeError(f"pykrx get_market_ohlcv_by_ticker ê²°ê³¼ê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤. ({date_ymd})")

    # ì»¬ëŸ¼ ì´ë¦„ í†µì¼
    rename_map = {
        "ì‹œê°€": "Open",
        "ê³ ê°€": "High",
        "ì €ê°€": "Low",
        "ì¢…ê°€": "Close",
        "ê±°ë˜ëŸ‰": "Volume",
        "ë“±ë½ë¥ ": "ChangePct",
    }
    df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})

    # ChangePctê°€ ìˆìœ¼ë©´ Change(ì†Œìˆ˜)ë¡œ ë³€í™˜
    if "ChangePct" in df.columns:
        df["Change"] = df["ChangePct"] / 100.0
    else:
        df["Change"] = 0.0

    # ì¸ë±ìŠ¤(í‹°ì»¤)ë¥¼ Codeë¡œ
    df = df.reset_index().rename(columns={"í‹°ì»¤": "Code"})

    # Name ì±„ìš°ê¸° (í•œ ë²ˆë§Œ í˜¸ì¶œë˜ë¯€ë¡œ ì†ë„ ê´œì°®ìŒ)
    def _get_name_safe(ticker: str) -> str:
        try:
            return stock.get_market_ticker_name(ticker)
        except Exception:
            return ""

    df["Name"] = df["Code"].map(_get_name_safe)

    # Date ê³ ì •
    df["Date"] = date

    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì •ë¦¬
    keep_cols = ["Date", "Open", "High", "Low", "Close", "Volume", "Change", "Code", "Name"]
    for col in keep_cols:
        if col not in df.columns:
            df[col] = pd.NA
    df = df[keep_cols]

    # ì˜ì‹¬ ì¢…ëª© ì°¾ê¸°: OHLCV ê°€ NaNì´ê±°ë‚˜ 0/ìŒìˆ˜ì´ë©´ ë°±ì—… í›„ë³´
    mask_bad = _invalid_ohlcv_mask(df)
    suspicious_codes = df.loc[mask_bad, "Code"].tolist()

    log(
        f"[KRX] {date_ymd}: {len(df)}ê°œ ì¢…ëª© ìˆ˜ì§‘ ì™„ë£Œ, "
        f"ì˜ì‹¬ ì¢…ëª©(OHLCV NaN/0) {len(suspicious_codes)}ê°œ"
    )
    return df, suspicious_codes

def fill_missing_with_sources(
    daily_df: pd.DataFrame,
    date: dt.date,
    codes: List[str],
    sources: Optional[Iterable[Tuple[str, Callable[[str, str], Optional[dict]]]]] = None,
) -> Tuple[pd.DataFrame, List[str]]:
    """pykrxì—ì„œ ë¹ ì§„ í‹°ì»¤ë¥¼ ì—¬ëŸ¬ ì†ŒìŠ¤ë¡œ êµì°¨ ê²€ì¦í•´ ë³´ì •í•œë‹¤."""
    if not codes:
        return daily_df, []

    if sources is None:
        sources = FALLBACK_SOURCES

    date_ymd = to_ymd(date)
    missing_log_path = os.path.join(LOG_DIR, f"missing_{date_ymd}.txt")
    unresolved = list(dict.fromkeys(codes))

    for source_name, fetcher in sources:
        if not unresolved:
            break

        log(f"[{source_name.upper()}] ê²°ì¸¡ ë³´ì • ì‹œë„ (ì”ì—¬ {len(unresolved)}ê±´)")
        next_round: List[str] = []
        progress_active = False

        for idx, code in enumerate(unresolved, start=1):
            progress_active = True
            inline_progress(f"    [{source_name.upper()}] {idx}/{len(unresolved)} {code}")
            info = fetcher(code, date_ymd)
            if not info:
                next_round.append(code)
                continue

            mask = daily_df["Code"] == code
            if not mask.any():
                row = {
                    "Date": info["Date"],
                    "Open": info["Open"],
                    "High": info["High"],
                    "Low": info["Low"],
                    "Close": info["Close"],
                    "Volume": info["Volume"],
                    "Change": 0.0,
                    "Code": code,
                    "Name": stock.get_market_ticker_name(code),
                }
                daily_df = pd.concat([daily_df, pd.DataFrame([row])], ignore_index=True)
                mask = daily_df["Code"] == code

            for col in OHLCV_COLS:
                series = daily_df.loc[mask, col]
                if series.isna().any() or (series <= 0).any():
                    daily_df.loc[mask, col] = info[col]

            if _invalid_ohlcv_mask(daily_df.loc[mask, OHLCV_COLS]).any():
                next_round.append(code)

        if progress_active:
            end_inline_progress()

        resolved_count = len(unresolved) - len(next_round)
        log(f"    -> {source_name} ë³´ì • ì„±ê³µ {resolved_count}ê±´, ì”ì—¬ {len(next_round)}ê±´")
        unresolved = next_round

    if unresolved:
        with open(missing_log_path, "w", encoding="utf-8") as f:
            f.write("\n".join(unresolved))
        log(f"[WARN] êµì°¨ ê²€ì¦ì—ì„œë„ ì‹¤íŒ¨í•œ ì¢…ëª© {len(unresolved)}ê±´ - {missing_log_path} ê¸°ë¡")
    else:
        if os.path.exists(missing_log_path):
            os.remove(missing_log_path)
        log("[FALLBACK] ëª¨ë“  ê²°ì¸¡ ì¢…ëª©ì„ ë³´ì •í–ˆìŠµë‹ˆë‹¤")

    return daily_df, unresolved


# ===== ë©”ì¸ ì‹¤í–‰ =====
def main():
    print_header()

    # 1) RAW íŒŒì¼ ë¡œë“œ + ê¸°ë³¸ ì •ë³´ ì¶œë ¥
    raw_df = load_raw_main()
    last_date = max(raw_df["Date"])
    latest_mask = raw_df["Date"] == last_date
    latest_codes = raw_df.loc[latest_mask, "Code"].nunique()
    universe_codes = raw_df["Code"].nunique()

    print(f"[STEP 1] í˜„ì¬ RAW ìµœì‹  ë‚ ì§œ: {last_date}")
    print(f"         - ê¸°ì¤€ ì¢…ëª© ìˆ˜(ìµœì‹ ì¼ ê¸°ì¤€): {latest_codes}ê°œ")
    print(f"         - ì „ì²´ ìœ ë‹ˆë²„ìŠ¤ ì¢…ëª© ìˆ˜   : {universe_codes}ê°œ")

    # 2) ì—…ë°ì´íŠ¸ ëŒ€ìƒ ë²”ìœ„ ê³„ì‚° (í•­ìƒ 'ì–´ì œ'ê¹Œì§€)
    today = dt.datetime.now()
    today_date = today.date()
    cutoff_date = get_prev_bizdate(today_date)
    log(f"[INFO] ì´ë²ˆ ì‹¤í–‰ì˜ ìˆ˜ì§‘ ìƒí•œ(ì–´ì œ ê¸°ì¤€ ì˜ì—…ì¼): {cutoff_date}")

    if last_date >= cutoff_date:
        log(
            f"[INFO] RAW ìµœì‹  ë‚ ì§œ({last_date})ê°€ ìˆ˜ì§‘ ìƒí•œ({cutoff_date}) ì´ìƒì…ë‹ˆë‹¤. "
            "ì´ë²ˆ ì‹¤í–‰ì—ì„œ ìƒˆë¡œ ë°›ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        )
        return

    target_dates: List[dt.date] = []
    next_date = get_next_bizdate(last_date)
    while next_date <= cutoff_date:
        target_dates.append(next_date)
        next_date = get_next_bizdate(next_date)

    print()
    print(
        f"[STEP 2] ìˆ˜ì§‘ ëŒ€ìƒ ê¸°ê°„: {target_dates[0]} ~ {target_dates[-1]} "
        f"(ì´ {len(target_dates)}ì˜ì—…ì¼)"
    )
    print()

    # 3) RAW ë°±ì—…
    log("[STEP 3] RAW ë°±ì—… ìƒì„±")
    backup_path = backup_raw_main(raw_df, today_date)
    log(f"[BACKUP] RAW ë°±ì—… ìƒì„±: {backup_path}")

    # 4) ë‚ ì§œë³„ íŒ¨ì¹˜ ë£¨í”„
    for idx, target_date in enumerate(target_dates, start=1):
        log(f"[STEP 4-{idx}] {target_date} ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        daily_df, suspicious_codes = build_daily_from_pykrx(target_date)
        daily_df, unresolved = fill_missing_with_sources(daily_df, target_date, suspicious_codes)

        target_ymd = to_ymd(target_date)
        daily_path = os.path.join(DAILY_DIR, f"{target_ymd}.parquet")
        daily_df.to_parquet(daily_path)
        log(f"[DAILY ì €ì¥] {daily_path}")

        if unresolved:
            log(
                f"[WARN] {target_ymd} ê¸°ì¤€ êµì°¨ ê²€ì¦ ì‹¤íŒ¨ ì¢…ëª© {len(unresolved)}ê±´ - "
                "LOGS í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”."
            )

        raw_df = merge_daily_into_raw(raw_df, daily_df)
        log(f"[STEP 4-{idx}] ë³‘í•© ì™„ë£Œ - ëˆ„ì  ìµœì‹  ë‚ ì§œ {max(raw_df['Date'])}")

    # 5) RAW ì €ì¥
    raw_df.to_parquet(RAW_MAIN)

    log("ğŸ‰ [ì™„ë£Œ] RAW íŒ¨ì¹˜ ë° ë³‘í•© ì™„ë£Œ")
    log(f"    - ìµœì‹  ë‚ ì§œ : {max(raw_df['Date'])}")
    log(f"    - ì´ í–‰ ìˆ˜   : {len(raw_df):,}")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n[INTERRUPTED] ì‚¬ìš©ìê°€ ê°•ì œ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print("\n[ERROR] ì˜ˆì™¸ ë°œìƒ:", e)
        sys.exit(1)
