# raw_patch.py  (V6 - Kiwoom 1st Source Added)
# ìˆ˜ì •ì‚¬í•­:
# 1. build_daily_from_pykrx ë“± ëˆ„ë½ëœ í•¨ìˆ˜ ë³µêµ¬
# 2. ì´ë¯¸ ìµœì‹  ë°ì´í„°(target_date == last_date)ê°€ ìˆìœ¼ë©´ ìˆ˜ì§‘ SKIP ê¸°ëŠ¥ ì¶”ê°€
# 3. íŒŒì¼ ì €ì¥ ì‹œ ë‚ ì§œ íƒœê·¸ ê·œì¹™ ì¤€ìˆ˜

import os
import sys
import time
import math
import datetime as dt
from functools import lru_cache
from typing import Optional, Tuple, List, Iterable, Callable

import pandas as pd
from pykrx import stock
import requests
import FinanceDataReader as fdr
import glob
import os as _os
import yfinance as yf
import os.path as _path

# ============================================================
#  KIWOOM REST API ê²½ë¡œ/ëª¨ë“ˆ ì„¤ì •
# ============================================================
KIWOOM_REST_DIR = r"F:\autostockG"
if KIWOOM_REST_DIR not in sys.path:
    sys.path.append(KIWOOM_REST_DIR)

# REST API ì „ìš© ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸°
try:
    from kiwoom_rest.token_manager import KiwoomTokenManager
    from kiwoom_rest.kiwoom_api import KiwoomRestApi
except ImportError:
    print("Warning: kiwoom_rest ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

# ======================
# ê²½ë¡œ ì„¤ì •
# ======================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
STOCKS_DIR = os.path.join(BASE_DIR, "stocks")
RAW_MAIN = os.path.join(STOCKS_DIR, "all_stocks_cumulative.parquet")
DAILY_DIR = os.path.join(STOCKS_DIR, "DAILY")
LOG_DIR = os.path.join(STOCKS_DIR, "LOGS")
OHLCV_COLS = ["Open", "High", "Low", "Close", "Volume"]

os.makedirs(DAILY_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# MODELENGINE ë£¨íŠ¸ ê²½ë¡œë¥¼ sys.pathì— ì¶”ê°€í•˜ì—¬ UTIL ëª¨ë“ˆ ì‚¬ìš©
PROJECT_ROOT = os.path.abspath(os.path.join(BASE_DIR, ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

from UTIL.config_paths import versioned_filename
from UTIL.version_utils import save_dataframe_with_date, find_latest_file

def print_header():
    print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ ğŸ‰ í°ë‘¥ì´ ì›ë³¸ë°ì´í„° ì—…ë°ì´íŠ¸ (V6)           â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print(f"[PATH] RAW_MAIN : {RAW_MAIN}")
    print()

def log(msg: str):
    ts = dt.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{ts}] {msg}")

def _invalid_ohlcv_mask(df: pd.DataFrame) -> pd.Series:
    subset = df[OHLCV_COLS] if all(col in df.columns for col in OHLCV_COLS) else df
    return subset.isna().any(axis=1) | (subset <= 0).any(axis=1)

# ======================
# ë‚ ì§œ ìœ í‹¸
# ======================
def to_ymd(d: dt.date) -> str:
    return d.strftime("%Y%m%d")

def parse_date(s: str) -> dt.date:
    s = str(s)
    if "-" in s:
        return dt.datetime.strptime(s, "%Y-%m-%d").date()
    return dt.datetime.strptime(s, "%Y%m%d").date()

@lru_cache(maxsize=512)
def _nearest_bizday_cached(date_ymd: str) -> str:
    return stock.get_nearest_business_day_in_a_week(date_ymd)

def is_trading_day(date: dt.date) -> bool:
    today = dt.date.today()
    if date == today:
        return date.weekday() < 5

    date_ymd = to_ymd(date)
    try:
        nearest = _nearest_bizday_cached(date_ymd)
        if nearest == date_ymd:
            return True
    except Exception as e:
        log(f"[WARN] pykrx ë‚ ì§œ í™•ì¸ ì‹¤íŒ¨ - {e}. ì£¼ë§ ì—¬ë¶€ë¡œë§Œ íŒì •.")

    # pykrx ì‹¤íŒ¨ ì‹œ FDR(ì½”ìŠ¤í”¼ ì§€ìˆ˜)ë¡œ íœ´ì¼ ì—¬ë¶€ ë³´ì¡° í™•ì¸
    try:
        df_tmp = fdr.DataReader("KS11", date, date)
        if df_tmp is not None and not df_tmp.empty:
            return True
    except Exception as e:
        log(f"[WARN] FDR íœ´ì¼ í™•ì¸ ì‹¤íŒ¨ - {e}. ìš”ì¼ë§Œ ì‚¬ìš©.")

    return date.weekday() < 5

def get_next_bizdate(last_date: dt.date) -> dt.date:
    d = last_date
    for _ in range(400):
        d = d + dt.timedelta(days=1)
        if is_trading_day(d):
            return d
    raise RuntimeError("ë‹¤ìŒ ì˜ì—…ì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

# ======================
# SAVE / LOAD
# ======================
def load_raw_main() -> pd.DataFrame:
    # find_latest_fileì„ ì‚¬ìš©í•˜ì—¬ ìµœì‹  ë‚ ì§œ íƒœê·¸ íŒŒì¼ ë¡œë“œ
    latest_path = find_latest_file(STOCKS_DIR, "all_stocks_cumulative")
    
    if latest_path and os.path.exists(latest_path):
        log(f"[INFO] ìµœì‹  RAW íŒŒì¼ ì‚¬ìš©: {os.path.basename(latest_path)}")
        df = pd.read_parquet(latest_path)
    else:
        # íƒœê·¸ íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë ˆê±°ì‹œ íŒŒì¼ í™•ì¸
        if os.path.exists(RAW_MAIN):
            log(f"[INFO] íƒœê·¸ íŒŒì¼ ì—†ìŒ. ê¸°ì¡´ RAW_MAIN ì‚¬ìš©: {os.path.basename(RAW_MAIN)}")
            df = pd.read_parquet(RAW_MAIN)
        else:
            raise FileNotFoundError(f"RAW ë©”ì¸ íŒŒì¼ ì—†ìŒ: {RAW_MAIN} ë˜ëŠ” all_stocks_cumulative_*.parquet")

    df["Date"] = pd.to_datetime(df["Date"]).dt.date
    return df

def merge_daily_into_raw(raw_df: pd.DataFrame, daily_df: pd.DataFrame) -> pd.DataFrame:
    merged = pd.concat([raw_df, daily_df], ignore_index=True)
    merged = merged.drop_duplicates(subset=["Date", "Code"], keep="last")
    merged = merged.sort_values(["Date", "Code"]).reset_index(drop=True)
    return merged

# =====================================================================================
# [ë³µêµ¬] Kiwoom REST API ìˆ˜ì§‘ í•¨ìˆ˜
# =====================================================================================
def build_daily_from_kiwoom(date: dt.date, tickers: Optional[List[str]] = None) -> Tuple[pd.DataFrame, List[str]]:
    log(f"[STEP] KIWOOM ì „ì²´ ì¼ë´‰ ìˆ˜ì§‘ ì‹œì‘: {to_ymd(date)}")

    import configparser
    cfg = configparser.ConfigParser()
    cfg.read(r"F:\autostockG\kiwoom_rest\config.ini")

    MODE = cfg["SETTINGS"]["MODE"].strip().lower()
    BASE_URL = cfg["SETTINGS"]["BASE_URL_PAPER"] if MODE == "paper" else cfg["SETTINGS"]["BASE_URL"]

    token_mgr = KiwoomTokenManager(
        config_file=r"F:\autostockG\kiwoom_rest\config.ini",
        token_file=r"F:\autostockG\kiwoom_rest\token.json"
    )
    token = token_mgr.get_access_token()

    if tickers is None:
        try:
            tickers = stock.get_market_ticker_list(date=to_ymd(date), market="ALL")
        except Exception:
            tickers = []
        if not tickers:
            try:
                tickers = load_raw_main()["Code"].unique().tolist()
            except Exception:
                tickers = []

    rows = []
    bad_codes = []
    target = to_ymd(date)
    timeout_sec = 2 

    for idx, code in enumerate(tickers, 1):
        if idx % 100 == 0:
            log(f"[KIWOOM] ì§„í–‰ {idx}/{len(tickers)}")

        url = f"{BASE_URL}/api/dostk/chart"
        headers = {
            "Content-Type": "application/json;charset=UTF-8",
            "api-id": "ka10081",
            "authorization": f"Bearer {token}",
        }
        body = {
            "stk_cd": code,
            "base_dt": target,
            "upd_stkpc_tp": "0",
        }

        try:
            r = requests.post(url, headers=headers, json=body, timeout=timeout_sec)
            r.raise_for_status()
            js = r.json()

            chart = js.get("stk_dt_pole_chart_qry") or js.get("chart") or []
            if not chart:
                bad_codes.append(code)
                continue
            matched = None
            for item in chart:
                if str(item.get("dt")) == target:
                    matched = item
                    break
            if matched is None:
                matched = chart[0]

            def _to_float(v):
                try:
                    return float(str(v).replace("+", "").replace(",", "").strip())
                except Exception:
                    return float("nan")

            rows.append({
                "Date": date,
                "Open": _to_float(matched.get("open_pric")),
                "High": _to_float(matched.get("high_pric")),
                "Low": _to_float(matched.get("low_pric")),
                "Close": _to_float(matched.get("cur_prc")),
                "Volume": _to_float(matched.get("trde_qty")),
                "Change": 0.0,
                "Code": code,
                "Name": "",
                "Market": ""
            })

        except Exception:
            bad_codes.append(code)
            continue

    df = pd.DataFrame(rows)
    log(f"[KIWOOM] {len(df)}ê°œ ì¢…ëª© ìˆ˜ì§‘, ì‹¤íŒ¨ {len(bad_codes)}ê°œ")
    return df, bad_codes

# =====================================================================================
# [ë³µêµ¬] pykrx ìˆ˜ì§‘ í•¨ìˆ˜ (ì´ê²Œ ì—†ì–´ì„œ ì—ëŸ¬ë‚¬ì—ˆìŒ)
# =====================================================================================
def build_daily_from_pykrx(date: dt.date) -> Tuple[pd.DataFrame, List[str]]:
    for key in ["http_proxy", "https_proxy", "HTTP_PROXY", "HTTPS_PROXY"]:
        _os.environ[key] = ""
    date_ymd = to_ymd(date)
    log(f"[STEP] KRX ì¼ê´„ ìˆ˜ì§‘ ì‹œì‘: {date_ymd}")

    df = stock.get_market_ohlcv_by_ticker(date_ymd, market="ALL")
    if df is None or df.empty:
        raise RuntimeError(f"KRX ë°ì´í„° ì—†ìŒ: {date_ymd}")

    eng_map = {"Open": "ì‹œê°€", "High": "ê³ ê°€", "Low": "ì €ê°€", "Close": "ì¢…ê°€", "Volume": "ê±°ë˜ëŸ‰"}
    if set(eng_map.keys()).issubset(df.columns):
        df = df.rename(columns=eng_map)

    rename_map = {
        "ì‹œê°€": "Open", "ê³ ê°€": "High", "ì €ê°€": "Low",
        "ì¢…ê°€": "Close", "ê±°ë˜ëŸ‰": "Volume", "ë“±ë½ë¥ ": "ChangePct"
    }
    df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})

    if "ChangePct" in df.columns:
        df["Change"] = df["ChangePct"] / 100.0
    else:
        df["Change"] = 0.0

    df = df.reset_index().rename(columns={"í‹°ì»¤": "Code"})

    def _get_name_safe(ticker: str) -> str:
        try:
            return stock.get_market_ticker_name(ticker)
        except:
            return ""

    df["Name"] = df["Code"].map(_get_name_safe)
    df["Date"] = date

    keep_cols = ["Date","Open","High","Low","Close","Volume","Change","Code","Name"]
    for col in keep_cols:
        if col not in df.columns:
            df[col] = pd.NA
    df = df[keep_cols]

    mask_bad = _invalid_ohlcv_mask(df)
    suspicious_codes = df.loc[mask_bad, "Code"].tolist()

    log(f"[KRX] {len(df)}ê°œ ì¢…ëª© ìˆ˜ì§‘, ì˜ì‹¬ {len(suspicious_codes)}ê°œ")
    return df, suspicious_codes

# =====================================================================================
# [ë³µêµ¬] Fallback ì†ŒìŠ¤ í•¨ìˆ˜ë“¤
# =====================================================================================
def fetch_ohlcv_from_naver(ticker: str, yyyymmdd: str) -> Optional[dict]:
    url = f"https://api.finance.naver.com/siseJson.naver?symbol={ticker}&requestType=1&startTime={yyyymmdd}&endTime={yyyymmdd}"
    try:
        r = requests.get(url, timeout=5)
        arr = r.json()
        if not arr or len(arr) < 2:
            return None
        row = arr[1]
        return {
            "Open": float(row[1]), "High": float(row[2]),
            "Low": float(row[3]), "Close": float(row[4]),
            "Volume": float(row[5]),
        }
    except:
        return None

def fetch_ohlcv_from_fdr(ticker: str, yyyymmdd: str) -> Optional[dict]:
    try:
        start = dt.datetime.strptime(yyyymmdd, "%Y%m%d").date()
        end = start + dt.timedelta(days=1)
        df = fdr.DataReader(ticker, start, end)
        if df is None or df.empty:
            return None
        row = df.iloc[0]
        return {
            "Open": float(row["Open"]), "High": float(row["High"]),
            "Low": float(row["Low"]), "Close": float(row["Close"]),
            "Volume": float(row["Volume"]),
        }
    except:
        return None

def fetch_ohlcv_from_yahoo(ticker: str, yyyymmdd: str) -> Optional[dict]:
    try:
        dt_date = dt.datetime.strptime(yyyymmdd, "%Y%m%d").date()
        start = dt_date.strftime("%Y-%m-%d")
        end = (dt_date + dt.timedelta(days=1)).strftime("%Y-%m-%d")
        for suffix in [".KS", ".KQ", ""]:
            df = yf.download(f"{ticker}{suffix}", start=start, end=end, progress=False)
            if df is None or df.empty:
                continue
            row = df.iloc[0]
            return {
                "Open": float(row["Open"]), "High": float(row["High"]),
                "Low": float(row["Low"]), "Close": float(row["Close"]),
                "Volume": float(row["Volume"]),
            }
        return None
    except:
        return None

FALLBACK_SOURCES: Iterable[Tuple[str, Callable[[str,str], Optional[dict]]]] = [
    ("fdr", fetch_ohlcv_from_fdr),
    ("yahoo", fetch_ohlcv_from_yahoo),
    ("naver", fetch_ohlcv_from_naver),
]

def fill_missing_with_sources(daily_df: pd.DataFrame, date: dt.date, codes: List[str],
                              sources: Optional[Iterable[Tuple[str,Callable[[str,str],Optional[dict]]]]] = None):
    if not codes:
        return daily_df, []
    if sources is None:
        sources = FALLBACK_SOURCES

    date_ymd = to_ymd(date)
    unresolved = list(dict.fromkeys(codes))

    for source_name, fetcher in sources:
        if not unresolved:
            break
        log(f"[{source_name.upper()}] ë³´ì¡° ìˆ˜ì§‘ ì‹œì‘, {len(unresolved)}ê°œ ë‚¨ìŒ")

        still = []
        rows_update = {}

        for code in unresolved:
            o = fetcher(code, date_ymd)
            if not o:
                still.append(code)
                continue
            rows_update[code] = o

        for code, o in rows_update.items():
            idx = daily_df.index[daily_df["Code"] == code].tolist()
            if idx:
                i = idx[0]
                for col in ["Open","High","Low","Close","Volume"]:
                    daily_df.at[i, col] = o[col]
                daily_df.at[i, "Change"] = 0.0

        unresolved = still
        log(f"[{source_name.upper()}] ì²˜ë¦¬ í›„ ë‚¨ì€ ì½”ë“œ: {len(unresolved)}")

    return daily_df, unresolved


def build_daily_from_fallback_sources(date: dt.date, tickers: Iterable[str]) -> Tuple[pd.DataFrame, List[str]]:
    tickers = list(dict.fromkeys(tickers))
    base = pd.DataFrame({"Code": tickers})
    base["Date"] = date
    for col in ["Open","High","Low","Close","Volume","Change","Name"]:
        if col not in base.columns:
            base[col] = pd.NA

    daily_df, unresolved = fill_missing_with_sources(base, date, tickers, FALLBACK_SOURCES)
    mask_bad = _invalid_ohlcv_mask(daily_df)
    unresolved = list(dict.fromkeys(unresolved + daily_df.loc[mask_bad, "Code"].tolist()))
    log(f"[FALLBACK] ì„±ê³µ {len(daily_df) - len(unresolved)}ê±´, ì‹¤íŒ¨ {len(unresolved)}ê±´")
    return daily_df, unresolved


# =====================================================================================
# â­ ë©”ì¸ ì‹¤í–‰ë¶€
# =====================================================================================
if __name__ == "__main__":
    print_header()

    now_dt = dt.datetime.now()
    today = now_dt.date()
    raw_df = load_raw_main()

    last_date = raw_df["Date"].max()
    log(f"[STEP 1] RAW ìµœì‹  ë‚ ì§œ: {last_date}")

    # 16~18ì‹œëŠ” ì˜¤ì—¼ ë°©ì§€ë¡œ ì¤‘ë‹¨
    if dt.time(16, 0) <= now_dt.time() < dt.time(18, 0):
        log("[WARN] 16~18ì‹œëŠ” ì˜¤ì—¼ ë°©ì§€ë¡œ ìˆ˜ì§‘ ì¤‘ë‹¨")
        sys.exit(0)

    try:
        now_t = now_dt.time()
        is_biz = is_trading_day(today)

        # 1) 16:00 ì´ì „ â†’ ë¬´ì¡°ê±´ ì „ì¼(last_date)
        if now_t < dt.time(16, 0):
            target_date = last_date

        # 2) 16:00~18:00 â†’ ìœ„ì—ì„œ ì°¨ë‹¨ë¨ (pass)

        # 3) 18:00 ì´í›„
        else:
            if is_biz:
                target_date = today
            else:
                target_date = last_date

        # 4) ë¯¸ë˜ ë‚ ì§œ ë°©ì§€
        if target_date > today:
            log("[SAFEGUARD] ë¯¸ë˜ ë‚ ì§œë¡œ ì í”„ ì°¨ë‹¨ â†’ last_dateë¡œ ì¡°ì •")
            target_date = last_date

    except Exception as e:
        log(f"[ERROR] ë‚ ì§œ íŒì • ì‹¤íŒ¨: {e}")
        sys.exit(1)

    log(f"[STEP 2] ìˆ˜ì§‘ ê¸°ê°„: {target_date} ~ {target_date}")
    date = target_date

    # -----------------------------------------------------------
    # [ìˆ˜ì •] ì´ë¯¸ í•´ë‹¹ ë‚ ì§œ ë°ì´í„°ê°€ ì¡´ì¬í•˜ë©´ SKIP (ë‹¨, ì¥ì¤‘ ì—…ë°ì´íŠ¸ëŠ” ê³ ë ¤ ì•ˆí•¨)
    # -----------------------------------------------------------
    if target_date == last_date:
        log(f"âœ… [SKIP] ì´ë¯¸ ìµœì‹  ë°ì´í„°({target_date})ê°€ RAW íŒŒì¼ì— ì¡´ì¬í•©ë‹ˆë‹¤.")
        log("   (ì¶”ê°€ ìˆ˜ì§‘ ì—†ì´ ì¢…ë£Œí•©ë‹ˆë‹¤.)")
        sys.exit(0)
    # -----------------------------------------------------------

    # ===========================
    # â­â­ 1ìˆœìœ„: pykrx
    # ===========================
    try:
        daily_df, bad_codes = build_daily_from_pykrx(date)
        log("[OK] KRX(pykrx) ë°ì´í„° ì‚¬ìš©")
    except Exception as e:
        log(f"[WARN] KRX(pykrx) ì‹¤íŒ¨ â†’ {e}")
        daily_df = None
        bad_codes = []

    # ===========================
    # â­â­ 2ìˆœìœ„: KIWOOM
    # ===========================
    if daily_df is None or daily_df.empty:
        try:
            daily_df, bad_codes = build_daily_from_kiwoom(date)
            log("[OK] KIWOOM ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µ. 2ìˆœìœ„ ì†ŒìŠ¤ë¡œ ì‚¬ìš©.")
        except Exception as e:
            log(f"[WARN] KIWOOM ì‹¤íŒ¨ â†’ {e}")
            daily_df = None
            bad_codes = []

    # â­â­ 3ìˆœìœ„: FDR/Yahoo/Naver ì „ì²´ ìˆ˜ì§‘
    if daily_df is None or daily_df.empty:
        tickers = raw_df["Code"].unique().tolist()
        daily_df, bad_codes = build_daily_from_fallback_sources(date, tickers)
        if daily_df is None or daily_df.empty:
            log("[ERROR] FDR/Yahoo/Naver fallback ë„ ì‹¤íŒ¨")
            sys.exit(1)
        else:
            log("[OK] FDR/Yahoo/Naver fallback ì‚¬ìš©")

    # ===========================
    # â­â­ ë¶€ì¡±ë¶„ ë³´ì¡° ìˆ˜ì§‘
    # ===========================
    if bad_codes:
        try:
            # ìë™ ì‹¤í–‰ í™˜ê²½ì—ì„œëŠ” y ì…ë ¥ ì—†ì´ ìë™ ì²˜ë¦¬í•˜ê±°ë‚˜, ê¸°ë³¸ê°’ ì‚¬ìš©
            # ans = input(...) -> ê°•ì œ ì§„í–‰
            pass 
        except Exception:
            pass

        # (1) Kiwoomìœ¼ë¡œ ì˜ì‹¬ ì½”ë“œ ë³´ì™„ ì‹œë„
        try:
            kiw_df, _ = build_daily_from_kiwoom(date, tickers=bad_codes)
            if kiw_df is not None and not kiw_df.empty:
                kiw_sub = kiw_df[kiw_df["Code"].isin(bad_codes)]
                if not kiw_sub.empty:
                    daily_df = merge_daily_into_raw(daily_df, kiw_sub)
                    log(f"[KIWOOM] ë³´ì¡° ìˆ˜ì§‘ìœ¼ë¡œ {len(kiw_sub)}ê°œ ë®ì–´ì”€")
        except Exception as e:
            log(f"[WARN] KIWOOM ë³´ì¡° ìˆ˜ì§‘ ì‹¤íŒ¨ â†’ {e}")

    # ===========================
    # SAVE
    # ===========================
    out_path = os.path.join(DAILY_DIR, f"daily_{to_ymd(date)}.parquet")
    daily_df.to_parquet(out_path)
    log(f"[SAVE] DAILY ì €ì¥ ì™„ë£Œ: {out_path}")

    merged = merge_daily_into_raw(raw_df, daily_df)
    
    # [ìˆ˜ì •] ë‚ ì§œ íƒœê·¸ ì €ì¥ ë¡œì§ ì‚¬ìš© (RAW_MAIN ë®ì–´ì“°ê¸° ëŒ€ì‹ )
    saved_path = save_dataframe_with_date(merged, STOCKS_DIR, "all_stocks_cumulative", date_col="Date")
    if saved_path:
        log(f"[SAVE] RAW ìµœì‹ ë³¸ ì €ì¥: {os.path.basename(saved_path)}")
    else:
        log("[SKIP] RAW ìµœì‹ ë³¸ ì €ì¥ ê±´ë„ˆëœ€ (ë™ì¼ ë‚ ì§œ íŒŒì¼ ì¡´ì¬)")

    log("[DONE] RAW ì—…ë°ì´íŠ¸ ë.")