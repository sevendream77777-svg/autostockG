# raw_patch_final.py (ìµœì¢… í†µí•© ì™„ì„±ë³¸)
# ì—­í• : RAW ë°ì´í„°ì˜ ìµœì‹  ë‚ ì§œ í™•ì¸, ë°ì´í„° ìˆ˜ì§‘, ì¤‘ë³µ ì œê±° ë° ë³‘í•©ì„ ìë™ ìˆ˜í–‰í•©ë‹ˆë‹¤.

import os
import sys
import time
from datetime import datetime, timedelta, date
from typing import Optional, Tuple, List, Dict, Any

import pandas as pd
import numpy as np

# --- ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ (í•„ìˆ˜) ---
# pip install yfinance requests pykrx pandas numpy ê°€ í•„ìš”í•©ë‹ˆë‹¤.
import requests
import yfinance as yf

try:
    from pykrx import stock as krx_stock
    HAS_KRX = True
except Exception:
    HAS_KRX = False

# ---------------------------------------------------------
# 1. ê²½ë¡œ ì„¤ì • ë° ê³µí†µ ë³€ìˆ˜
# ---------------------------------------------------------

RAW_MAIN = r"F:\autostockG\MODELENGINE\RAW\stocks\all_stocks_cumulative.parquet"
STOCKS_DIR = os.path.dirname(RAW_MAIN)

START_DATE = "2015-01-01"

# ---------------------------------------------------------
# 2. ë¡œê¹… í•¨ìˆ˜
# ---------------------------------------------------------

def log(msg: str):
    """ì½˜ì†” ë¡œê·¸ìš©"""
    print(msg, flush=True)

# ---------------------------------------------------------
# 3. ë°ì´í„° ìˆ˜ì§‘ ì—”ì§„ (Full Integration)
# ---------------------------------------------------------

def fetch_from_yahoo(code: str, start: str, end: str) -> pd.DataFrame:
    """1ì°¨: Yahoo Finance ìˆ˜ì§‘"""
    for suffix in [".KS", ".KQ"]:
        ticker = f"{code}{suffix}"
        try:
            df = yf.download(ticker, start=start, end=end, progress=False)
            if df is not None and not df.empty:
                df = df.reset_index()
                df["Code"] = code
                df = df.rename(columns={
                    "Date": "Date", "Open": "Open", "High": "High",
                    "Low": "Low", "Close": "Close", "Volume": "Volume"
                })
                df = df[["Date", "Code", "Open", "High", "Low", "Close", "Volume"]]
                df["Date"] = pd.to_datetime(df["Date"])
                log(f"  [YAHOO] {code} ì„±ê³µ.")
                return df
        except Exception:
            pass
    return pd.DataFrame()


def fetch_from_naver(code: str, start: str, end: str) -> pd.DataFrame:
    """2ì°¨: Naver ì°¨íŠ¸ API ìˆ˜ì§‘"""
    url = f"https://api.stock.naver.com/stock/{code}/chart"
    params = {"period": "DAY", "count": "4000"} 
    try:
        resp = requests.get(url, params=params, timeout=5)
        if resp.status_code != 200:
            return pd.DataFrame()

        data = resp.json()
        if "chart" not in data or "tradePrice" not in data["chart"]:
            return pd.DataFrame()

        dates = data["chart"]["time"]
        opens = data["chart"]["openingPrice"]
        highs = data["chart"]["highPrice"]
        lows = data["chart"]["lowPrice"]
        closes = data["chart"]["tradePrice"]
        volumes = data["chart"]["candleAccTradeVolume"]

        df = pd.DataFrame({
            "Date": pd.to_datetime(dates).date,
            "Open": opens,
            "High": highs,
            "Low": lows,
            "Close": closes,
            "Volume": volumes,
        })
        df["Code"] = code
        df["Date"] = pd.to_datetime(df["Date"])
        df = df[["Date", "Code", "Open", "High", "Low", "Close", "Volume"]]

        mask = (df["Date"] >= pd.to_datetime(start)) & (df["Date"] < pd.to_datetime(end))
        df = df.loc[mask].copy()

        if not df.empty:
            log(f"  [NAVER] {code} ì„±ê³µ.")
            return df
    except Exception:
        pass

    return pd.DataFrame()


def fetch_from_krx(code: str, start: str, end: str) -> pd.DataFrame:
    """3ì°¨: pykrx ì‚¬ìš©í•œ KRX ìˆ˜ì§‘"""
    if not HAS_KRX:
        return pd.DataFrame()

    try:
        start_krx = start.replace("-", "")
        end_krx = (pd.to_datetime(end) - pd.Timedelta(days=1)).strftime("%Y%m%d")

        df = krx_stock.get_market_ohlcv_by_date(start_krx, end_krx, code)
        if df is None or df.empty:
            return pd.DataFrame()

        df = df.reset_index()
        df = df.rename(columns={
            "ë‚ ì§œ": "Date", "ì‹œê°€": "Open", "ê³ ê°€": "High",
            "ì €ê°€": "Low", "ì¢…ê°€": "Close", "ê±°ë˜ëŸ‰": "Volume"
        })
        df["Code"] = code
        df["Date"] = pd.to_datetime(df["Date"])
        df = df[["Date", "Code", "Open", "High", "Low", "Close", "Volume"]]

        mask = (df["Date"] >= pd.to_datetime(start)) & (df["Date"] < pd.to_datetime(end))
        df = df.loc[mask].copy()

        if not df.empty:
            log(f"  [KRX] {code} ì„±ê³µ.")
            return df
    except Exception:
        pass

    return pd.DataFrame()


def fetch_ohlcv_multi_source(code: str, start: str, end: str,
                             fail_log: list,
                             fb_log: list,
                             krx_log: list) -> pd.DataFrame:
    """
    3ë‹¨ê³„ Fallback: Yahoo â†’ Naver â†’ KRX ìˆœìœ¼ë¡œ ì‹œë„.
    """
    df = fetch_from_yahoo(code, start, end)
    if df is not None and not df.empty:
        return df

    fb_log.append(code)
    df = fetch_from_naver(code, start, end)
    if df is not None and not df.empty:
        return df

    krx_log.append(code)
    df = fetch_from_krx(code, start, end)
    if df is not None and not df.empty:
        return df

    fail_log.append(code)
    return pd.DataFrame()

# ---------------------------------------------------------
# 4. ë°ì´í„° ì•ˆì •í™” ë¡œì§ (safe_raw_patch_v3 ê¸°ë°˜)
# ---------------------------------------------------------

def normalize_numeric_series(val):
    """ìˆ«ì ì»¬ëŸ¼ ì•ˆì •í™”"""
    if val is None:
        return pd.Series([pd.NA])
    if isinstance(val, pd.Series):
        return pd.to_numeric(val, errors="coerce")
    if isinstance(val, np.ndarray):
        val = val.flatten()
    if isinstance(val, (int, float, str)):
        val = [val]
    if isinstance(val, (list, tuple)):
        return pd.to_numeric(pd.Series(val), errors="coerce")
    return pd.to_numeric(pd.Series([val]), errors="coerce")


def fetch_single_day_multi(code: str, date_obj: date):
    """1ì¼ì¹˜ OHLCVë§Œ ìˆ˜ì§‘í•˜ì—¬ DataFrame ë°˜í™˜."""
    date_str = date_obj.strftime("%Y-%m-%d")
    start = date_str
    end = (date_obj + timedelta(days=1)).strftime("%Y-%m-%d")

    fail_log, fb_log, krx_log = [], [], []
    df_full = fetch_ohlcv_multi_source(code, start, end, fail_log, fb_log, krx_log)

    if df_full is None or df_full.empty:
        return None, "empty"

    df_full["Date"] = pd.to_datetime(df_full["Date"])
    df_day = df_full[df_full["Date"].dt.date == date_obj]

    if df_day.empty:
        return None, "empty"

    for col in ["Open", "High", "Low", "Close", "Volume"]:
        if col in df_day.columns:
            df_day[col] = normalize_numeric_series(df_day[col]).values

    df_day = df_day.dropna(subset=["Open", "High", "Low", "Close"])
    if df_day.empty:
        return None, "empty"

    return df_day, "success"

# ---------------------------------------------------------
# 5. RAW ìµœì‹  ë‚ ì§œ / ëŒ€ìƒ ë‚ ì§œ ê³„ì‚°
# ---------------------------------------------------------

def get_latest_raw_date(raw_path: str) -> Optional[date]:
    """RAW íŒŒì¼ì—ì„œ Date ì»¬ëŸ¼ì˜ ìµœëŒ“ê°’(ìµœì‹  ë‚ ì§œ)ì„ ë°˜í™˜"""
    if not os.path.exists(raw_path):
        return None
    df = pd.read_parquet(raw_path, columns=["Date"])
    if df.empty:
        return None
    return pd.to_datetime(df["Date"].max()).date()


def generate_missing_dates(latest: date, end_date: date) -> list:
    """
    latest+1ì¼ë¶€í„° end_dateê¹Œì§€ ì¤‘ ì˜ì—…ì¼ í›„ë³´ë“¤ì„ ìƒì„±.
    (ì£¼ë§ì€ 1ì°¨ í•„í„°ì—ì„œ ì œì™¸)
    """
    dates = []
    curr = latest + timedelta(days=1)
    while curr <= end_date:
        # 1ì°¨ í•„í„°: í† , ì¼ ì œì™¸
        if curr.weekday() < 5:
            dates.append(curr)
        curr += timedelta(days=1)
    return dates

# ---------------------------------------------------------
# 6. ì „ì²´ RAW ìë™ ì—…ë°ì´íŠ¸ ë¡œì§
# ---------------------------------------------------------

def auto_update_raw(target_end: Optional[str] = None):
    """
    RAW_MAIN ê¸°ì¤€ìœ¼ë¡œ ìµœì‹  ë‚ ì§œ ì´í›„ì˜ ë°ì´í„°ë¥¼ ìë™ ìˆ˜ì§‘/ë³‘í•©.

    1) RAW_MAINì—ì„œ ìµœì‹  ë‚ ì§œ ì½ìŒ
    2) ìµœì‹  ë‚ ì§œ + 1ì¼ ~ target_end (ë˜ëŠ” ì˜¤ëŠ˜) ê¹Œì§€ ë‚ ì§œ ëª©ë¡ ìƒì„±
    3) ê° ë‚ ì§œë³„ë¡œ ëª¨ë“  ì¢…ëª©ì— ëŒ€í•´ í•˜ë£¨ì¹˜ ë°ì´í„° ìˆ˜ì§‘
    4) ìˆ˜ì§‘ëœ ë°ì´í„°ë§Œ ê¸°ì¡´ RAWì™€ ë³‘í•©
    """

    log("===== RAW_PATCH.PY: ìë™ ì—…ë°ì´íŠ¸ ë° ë³‘í•© ì‹œì‘ =====")

    # [í´ë” ìƒì„±] stocks í´ë”ê°€ ì—†ìœ¼ë©´ ë§Œë“­ë‹ˆë‹¤.
    os.makedirs(STOCKS_DIR, exist_ok=True)

    # 1. RAW íŒŒì¼ ì¡´ì¬ í™•ì¸ ë° ìµœì‹  ë‚ ì§œ í™•ì¸
    if not os.path.exists(RAW_MAIN):
        log(f"[WARN] ë©”ì¸ RAW íŒŒì¼ ì—†ìŒ ({RAW_MAIN}). ì „ì²´ RAW êµ¬ì¶•ì´ ë¨¼ì € í•„ìš”í•©ë‹ˆë‹¤.")
        return

    latest_date = get_latest_raw_date(RAW_MAIN)
    if latest_date is None:
        log("[FATAL] RAW íŒŒì¼ì´ ë¹„ì—ˆê±°ë‚˜ ë‚ ì§œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‘ì—… ì¢…ë£Œ.")
        return

    log(f"[INFO] í˜„ì¬ RAW ìµœì‹  ë‚ ì§œ: {latest_date}")

    # 2. target_end ë‚ ì§œ ê²°ì • (ê¸°ë³¸: ì–´ì œê¹Œì§€)
    if target_end is None:
        today = date.today()
        target_end_date = today - timedelta(days=1)
    else:
        target_end_date = pd.to_datetime(target_end).date()

    if target_end_date <= latest_date:
        log(f"[INFO] ì´ë¯¸ ìµœì‹ ì…ë‹ˆë‹¤. (ëª©í‘œ: {target_end_date}, í˜„ì¬: {latest_date})")
        return

    fetch_dates = generate_missing_dates(latest_date, target_end_date)
    if not fetch_dates:
        log("[INFO] ìˆ˜ì§‘í•  ì¶”ê°€ ë‚ ì§œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    log(f"[INFO] ì‹ ê·œ ìˆ˜ì§‘ ëŒ€ìƒ ë‚ ì§œ ìˆ˜: {len(fetch_dates)}")
    log(f"      {fetch_dates[0]} ~ {fetch_dates[-1]}")

    # 3. ì¢…ëª© ì½”ë“œ ë¦¬ìŠ¤íŠ¸ í™•ë³´ (RAW íŒŒì¼ì—ì„œ)
    df_main = pd.read_parquet(RAW_MAIN, columns=["Date", "Code"])
    codes = sorted(df_main["Code"].unique())
    log(f"[INFO] ì¢…ëª© ìˆ˜: {len(codes)}ê°œ")

    all_new_data = []

    # 3-1. ë‚ ì§œë³„ë¡œ ë°˜ë³µ ìˆ˜ì§‘
    for date_obj in fetch_dates:
        # ... (ì´í•˜ ìˆ˜ì§‘ ë¡œì§ì€ ë™ì¼) ...
        log(f"\n[FETCH] ë‚ ì§œ: {date_obj.strftime('%Y-%m-%d')} ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
        all_rows_for_day = []
        n_success = 0

        for code in codes:
            df_day, status = fetch_single_day_multi(code, date_obj)

            if status == "success" and df_day is not None and not df_day.empty:
                all_rows_for_day.append(df_day)
                n_success += 1

        if n_success > 0 and all_rows_for_day:
            full_day_df = pd.concat(all_rows_for_day, ignore_index=True)
            log(f"[SUCCESS] {date_obj.strftime('%Y-%m-%d')}: {n_success}ê°œ ì¢…ëª© ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µ.")
            all_new_data.append(full_day_df)
        elif n_success == 0:
            log(f"[INFO] {date_obj.strftime('%Y-%m-%d')}: ê±°ë˜ì¼ ì•„ë‹˜ ë˜ëŠ” ìˆ˜ì§‘ ì‹¤íŒ¨ë¡œ ê±´ë„ˆëœ€.")

    if not all_new_data:
        log("[INFO] ìˆ˜ì§‘ëœ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‘ì—… ì¢…ë£Œ.")
        return

    # 4. ê¸°ì¡´ RAW ë¡œë“œ ë° ìƒˆë¡œìš´ ë°ì´í„°ì™€ ë³‘í•©
    df_main = pd.read_parquet(RAW_MAIN)
    frames = [df_main] + all_new_data

    merged = pd.concat(frames, ignore_index=True)

    # 5. ì¤‘ë³µ ì œê±° ë° ìµœì¢… ì •ë¦¬
    if "Date" not in merged.columns or "Code" not in merged.columns:
        log("[FATAL] ë³‘í•© ê²°ê³¼ì— Date/Code ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì‘ì—… ì¢…ë£Œ.")
        return

    merged["Date"] = pd.to_datetime(merged["Date"])
    merged = merged.drop_duplicates(subset=["Date", "Code"], keep="last")
    merged = merged.dropna(subset=["Date", "Code"])
    merged = merged.sort_values(["Date", "Code"]).reset_index(drop=True)

    # 6. ìµœì¢… ì €ì¥
    merged.to_parquet(RAW_MAIN)
    log("\nğŸ‰ [ì™„ë£Œ] RAW ìµœì¢… ì—…ë°ì´íŠ¸ ì™„ë£Œ.")
    log(f"       ê²½ë¡œ: {RAW_MAIN}")
    log(f"       ìµœì‹  ë‚ ì§œ: {merged['Date'].max().date()}, ì´ í–‰ìˆ˜: {len(merged):,}")
    log("===== RAW_PATCH.PY: ìë™ ì—…ë°ì´íŠ¸ ì™„ë£Œ =====")


if __name__ == "__main__":
    # ì˜ˆì‹œ: python raw_patch.py  â†’ ì–´ì œê¹Œì§€ ìë™ ì—…ë°ì´íŠ¸
    #       python raw_patch.py 2025-11-18  â†’ ì§€ì • ë‚ ì§œê¹Œì§€ ì—…ë°ì´íŠ¸
    if len(sys.argv) > 1:
        auto_update_raw(sys.argv[1])
    else:
        auto_update_raw()
