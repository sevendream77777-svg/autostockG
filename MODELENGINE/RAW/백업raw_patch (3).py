# raw_patch.py - V3 (ìµœì¢… í†µí•©ë³¸)
# ì—­í• :
#   1) ê¸°ì¡´ RAW(all_stocks_cumulative.parquet)ì˜ ìµœì‹  ë‚ ì§œë¥¼ ì°¾ê³ 
#   2) ê·¸ ë‹¤ìŒ ë‚ ì§œë¶€í„° ëª©í‘œ ë‚ ì§œ(ê¸°ë³¸: ì–´ì œ)ê¹Œì§€ í•˜ë£¨ ë‹¨ìœ„ë¡œ OHLCVë¥¼ ìˆ˜ì§‘í•œ ë’¤
#   3) ê²°ì¸¡/ëˆ„ë½ ê²€ì¦ì„ ìˆ˜í–‰í•˜ê³ 
#   4) DAILY ìŠ¤ëƒ…ìƒ· ì €ì¥ + RAW ë³¸ì²´ì— ë³‘í•©/ë°±ì—…ê¹Œì§€ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤.
#
# ë°ì´í„° ì†ŒìŠ¤ ìš°ì„ ìˆœìœ„:
#   Yahoo â†’ KRX(pykrx) â†’ Naver â†’ (stub) Kiwoom REST API

import os
import sys
import time
from datetime import datetime, timedelta, date
from typing import Optional, List, Tuple

import numpy as np
import pandas as pd

import requests
import yfinance as yf

try:
    from pykrx import stock as krx_stock
    HAS_KRX = True
except Exception:
    HAS_KRX = False

# ---------------------------------------------------------
# 1. ê²½ë¡œ ì„¤ì •
# ---------------------------------------------------------

# RAW ë©”ì¸ íŒŒì¼ (ìœ„ëŒ€í•˜ì‹ í˜¸ì •ë‹˜ í™˜ê²½ ê³ ì •)
RAW_MAIN = r"F:\autostockG\MODELENGINE\RAW\stocks\all_stocks_cumulative.parquet"

STOCKS_DIR = os.path.dirname(RAW_MAIN)
DAILY_DIR = os.path.join(STOCKS_DIR, "DAILY")   # í•˜ë£¨ ìŠ¤ëƒ…ìƒ·
LOG_DIR = os.path.join(STOCKS_DIR, "LOGS")      # ê²€ì¦/ì—ëŸ¬ ë¡œê·¸

os.makedirs(STOCKS_DIR, exist_ok=True)
os.makedirs(DAILY_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# ---------------------------------------------------------
# 2. ê³µí†µ ìœ í‹¸
# ---------------------------------------------------------

def log(msg: str) -> None:
    """ì½˜ì†” ì¶œë ¥ìš© ê°„ë‹¨ ë¡œê·¸"""
    print(msg, flush=True)


def backup_with_tag(path: str, tag: Optional[str] = None) -> Optional[str]:
    """
    ê¸°ì¡´ RAW íŒŒì¼ ë°±ì—….
    ê·œì¹™: íŒŒì¼ëª…_YYMMDD.parquet, ì¤‘ë³µ ì‹œ _YYMMDD_1, _YYMMDD_2 ...
    """
    if not os.path.exists(path):
        return None

    if tag is None:
        tag = datetime.today().strftime("%y%m%d")

    base_dir = os.path.dirname(path)
    base_name, ext = os.path.splitext(os.path.basename(path))

    candidate = os.path.join(base_dir, f"{base_name}_{tag}{ext}")
    idx = 1
    while os.path.exists(candidate):
        candidate = os.path.join(base_dir, f"{base_name}_{tag}_{idx}{ext}")
        idx += 1

    import shutil
    shutil.copy2(path, candidate)
    log(f"[BACKUP] RAW ë°±ì—… ìƒì„±: {candidate}")
    return candidate


def get_raw_latest_date(raw_path: str) -> Optional[date]:
    """RAW_MAINì—ì„œ Date ì»¬ëŸ¼ì˜ ìµœëŒ“ê°’(ìµœì‹  ë‚ ì§œ)ì„ ë°˜í™˜"""
    if not os.path.exists(raw_path):
        return None
    df = pd.read_parquet(raw_path, columns=["Date"])
    if df.empty:
        return None
    return pd.to_datetime(df["Date"]).max().date()


def generate_missing_dates(latest: date, end_date: date) -> List[date]:
    """
    latest+1ì¼ë¶€í„° end_dateê¹Œì§€ ì¤‘ ì£¼ë§ì„ ì œì™¸í•œ ë‚ ì§œ í›„ë³´ ìƒì„±.
    (ì‹¤ì œë¡œ íœ´ì¥ì¼ì¸ ê²½ìš°, ë‚˜ì¤‘ ë‹¨ê³„ì—ì„œ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìœ¼ë©´ ìë™ ê±´ë„ˆëœ€)
    """
    dates: List[date] = []
    curr = latest + timedelta(days=1)
    while curr <= end_date:
        if curr.weekday() < 5:  # ì›”(0)~ê¸ˆ(4)
            dates.append(curr)
        curr += timedelta(days=1)
    return dates


def normalize_numeric_series(val):
    """
    ìˆ«ì ì»¬ëŸ¼ ì•ˆì •í™”ìš© í—¬í¼.
    Series/ndarray/ìŠ¤ì¹¼ë¼ ëª¨ë‘ ì•ˆì „í•˜ê²Œ float ì‹œë¦¬ì¦ˆë¡œ ë³€í™˜.
    """
    if val is None:
        return pd.Series([pd.NA])
    if isinstance(val, pd.Series):
        return pd.to_numeric(val, errors="coerce")
    if isinstance(val, np.ndarray):
        val = val.flatten()
    if isinstance(val, (int, float, str)):
        val = [val]
    if isinstance(val, (list, tuple)):
        return pd.to_numeric(pd.Series(val), errors="coerce")
    return pd.to_numeric(pd.Series([val]), errors="coerce")


# ---------------------------------------------------------
# 3. ì›ì‹œ DF ì •ê·œí™”
# ---------------------------------------------------------

def normalize_raw_df(df: pd.DataFrame, code: str) -> pd.DataFrame:
    """
    ì–´ë–¤ ì„œë²„ì—ì„œ ì˜¤ë“  ì»¬ëŸ¼ì„ ê°•ì œë¡œ Date/Code/Open/High/Low/Close/Volume
    7ê°œì— ë§ì¶° ì •ê·œí™”.
    """
    if df is None or df.empty:
        return pd.DataFrame()

    # ì»¬ëŸ¼ ì´ë¦„ í‰íƒ„í™” (MultiIndex/ì´ìƒí•œ ì´ë¦„ ë°©ì§€)
    df.columns = [str(c).split(".")[-1] if isinstance(c, str) else str(c)
                  for c in df.columns]

    needed = ["Date", "Open", "High", "Low", "Close", "Volume"]
    for col in needed:
        if col not in df.columns:
            return pd.DataFrame()

    df["Date"] = pd.to_datetime(df["Date"], errors="coerce")
    df["Code"] = str(code).zfill(6)

    for col in ["Open", "High", "Low", "Close", "Volume"]:
        df[col] = pd.to_numeric(df[col], errors="coerce")

    df = df.dropna(subset=["Date"])
    df = df[["Date", "Code", "Open", "High", "Low", "Close", "Volume"]]

    return df


# ---------------------------------------------------------
# 4. ë°ì´í„° ì†ŒìŠ¤ë³„ ìˆ˜ì§‘ í•¨ìˆ˜
# ---------------------------------------------------------

def fetch_from_yahoo(code: str, start: str, end: str) -> pd.DataFrame:
    """1ì°¨: Yahoo Finance"""
    for suffix in [".KS", ".KQ"]:
        ticker = f"{code}{suffix}"
        try:
            df = yf.download(ticker, start=start, end=end, progress=False)
            if df is None or df.empty:
                continue
            df = df.reset_index()
            df = df.rename(columns={
                "Date": "Date",
                "Open": "Open",
                "High": "High",
                "Low": "Low",
                "Close": "Close",
                "Volume": "Volume",
            })
            norm = normalize_raw_df(df, code)
            if not norm.empty:
                log(f"      [YAHOO] {code} ({ticker}) âœ“ rows={len(norm)}")
                return norm
        except Exception as e:
            log(f"      [YAHOO] {code} ({ticker}) ì˜ˆì™¸: {e}")
    return pd.DataFrame()


def fetch_from_krx(code: str, start: str, end: str) -> pd.DataFrame:
    """2ì°¨: pykrx (KRX)"""
    if not HAS_KRX:
        return pd.DataFrame()

    try:
        s = start.replace("-", "")
        # pykrxëŠ” endê°€ 'í¬í•¨'ì´ ì•„ë‹ˆë¼ êµ¬ê°„ìœ¼ë¡œ ë™ì‘í•˜ë¯€ë¡œ, end-1ì¼ ì‚¬ìš©
        e = (pd.to_datetime(end) - pd.Timedelta(days=1)).strftime("%Y%m%d")

        df = krx_stock.get_market_ohlcv_by_date(s, e, code)
        if df is None or df.empty:
            return pd.DataFrame()
        df = df.reset_index()
        df = df.rename(columns={
            "ë‚ ì§œ": "Date",
            "ì‹œê°€": "Open",
            "ê³ ê°€": "High",
            "ì €ê°€": "Low",
            "ì¢…ê°€": "Close",
            "ê±°ë˜ëŸ‰": "Volume",
        })
        norm = normalize_raw_df(df, code)
        if not norm.empty:
            log(f"      [KRX ] {code} âœ“ rows={len(norm)}")
        return norm
    except Exception as e:
        log(f"      [KRX ] {code} ì˜ˆì™¸: {e}")
        return pd.DataFrame()


def fetch_from_naver(code: str) -> pd.DataFrame:
    """3ì°¨: Naver Stock API (ì „ì²´ íˆìŠ¤í† ë¦¬ ë°›ì•„ì„œ ë‚˜ì¤‘ì— ë‚ ì§œë¡œ í•„í„°)"""
    url = f"https://api.stock.naver.com/stock/{code}/chart"
    params = {"period": "DAY", "count": "5000"}
    try:
        r = requests.get(url, params=params, timeout=10)
        if r.status_code != 200:
            log(f"      [NAVER] {code} HTTP {r.status_code}")
            return pd.DataFrame()
        js = r.json()
        rows = js.get("chart", {}).get("result", [])
        if not rows:
            return pd.DataFrame()
        data = []
        for d in rows:
            dt = d.get("date")
            if isinstance(dt, str) and len(dt) == 8 and dt.isdigit():
                dt = f"{dt[:4]}-{dt[4:6]}-{dt[6:8]}"
            data.append([
                dt,
                code,
                d.get("open", 0),
                d.get("high", 0),
                d.get("low", 0),
                d.get("close", 0),
                d.get("volume", 0),
            ])
        df = pd.DataFrame(data, columns=["Date", "Code", "Open", "High",
                                         "Low", "Close", "Volume"])
        norm = normalize_raw_df(df, code)
        if not norm.empty:
            log(f"      [NAVER] {code} âœ“ rows={len(norm)}")
        return norm
    except Exception as e:
        log(f"      [NAVER] {code} ì˜ˆì™¸: {e}")
        return pd.DataFrame()


def fetch_from_kiwoom_stub(code: str, start: str, end: str) -> pd.DataFrame:
    """
    4ì°¨: Kiwoom REST API (í˜„ì¬ëŠ” STUB)
    ë‚˜ì¤‘ì— ìœ„ëŒ€í•˜ì‹ í˜¸ì •ë‹˜ì´ kiwoom_api.py ì˜ ì¼ë³„ OHLCV í•¨ìˆ˜ë¥¼ ì—°ê²°í•˜ë©´ ë¨.
    """
    # TODO: í•„ìš” ì‹œ kiwoom_api.get_daily_ohlcv(code, start, end) ì—°ê²°
    log(f"      [KIWOOM] {code} ì•„ì§ êµ¬í˜„ ì•ˆë¨ (stub)")
    return pd.DataFrame()


def fetch_ohlcv_multi_source(code: str, start: str, end: str) -> pd.DataFrame:
    """
    ë°ì´í„° ì†ŒìŠ¤ ìš°ì„ ìˆœìœ„:
      1) Yahoo
      2) KRX(pykrx)
      3) Naver
      4) Kiwoom(Stub)
    """
    # 1) Yahoo
    df = fetch_from_yahoo(code, start, end)
    if df is not None and not df.empty:
        return df

    # 2) KRX
    df = fetch_from_krx(code, start, end)
    if df is not None and not df.empty:
        return df

    # 3) Naver
    df = fetch_from_naver(code)
    if df is not None and not df.empty:
        # NaverëŠ” ì „ì²´ íˆìŠ¤í† ë¦¬ì´ë¯€ë¡œ ë‚ ì§œ ë²”ìœ„ë¡œ í•œ ë²ˆ ë” í•„í„°ë§
        start_dt = pd.to_datetime(start)
        end_dt = pd.to_datetime(end)
        mask = (df["Date"] >= start_dt) & (df["Date"] < end_dt)
        df = df.loc[mask]
        if not df.empty:
            return df

    # 4) Kiwoom stub
    df = fetch_from_kiwoom_stub(code, start, end)
    if df is not None and not df.empty:
        return df

    return pd.DataFrame()


# ---------------------------------------------------------
# 5. í•˜ë£¨ì¹˜ ë°ì´í„° ìˆ˜ì§‘ + ì •ê·œí™”
# ---------------------------------------------------------

def fetch_single_day_multi(code: str, date_obj: date) -> Tuple[Optional[pd.DataFrame], str]:
    """
    íŠ¹ì • ì¢…ëª©/í•˜ë£¨(date_obj)ì— ëŒ€í•œ OHLCV 1í–‰ì§œë¦¬ DataFrame ìˆ˜ì§‘.
    ë°˜í™˜: (df, status)  where status in {"success", "empty"}
    """
    date_str = date_obj.strftime("%Y-%m-%d")
    start = date_str
    end = (date_obj + timedelta(days=1)).strftime("%Y-%m-%d")

    df_full = fetch_ohlcv_multi_source(code, start, end)
    if df_full is None or df_full.empty:
        return None, "empty"

    df_full["Date"] = pd.to_datetime(df_full["Date"])
    df_day = df_full[df_full["Date"].dt.date == date_obj].copy()
    if df_day.empty:
        return None, "empty"

    # í•˜ë£¨ì— ì—¬ëŸ¬ í–‰ì´ ìˆì„ ê²½ìš° ê°€ì¥ ë§ˆì§€ë§‰(ë˜ëŠ” ì²« í–‰)ì„ ì‚¬ìš©
    df_day = df_day.sort_values("Date").tail(1)

    for col in ["Open", "High", "Low", "Close", "Volume"]:
        if col in df_day.columns:
            df_day[col] = normalize_numeric_series(df_day[col]).values

    df_day = df_day.dropna(subset=["Open", "High", "Low", "Close"])
    if df_day.empty:
        return None, "empty"

    df_day["Code"] = str(code).zfill(6)
    df_day["Date"] = pd.to_datetime(df_day["Date"])

    df_day = df_day[["Date", "Code", "Open", "High", "Low", "Close", "Volume"]]
    return df_day, "success"


# ---------------------------------------------------------
# 6. ê²€ì¦ ë¡œì§ (ëˆ„ë½ ì½”ë“œ/NaN ì²´í¬)
# ---------------------------------------------------------

def verify_day_against_baseline(date_obj: date,
                                day_df: pd.DataFrame,
                                baseline_codes: List[str]) -> None:
    """
    - baseline_codes: ìµœì‹  RAW ë‚ ì§œì— ì¡´ì¬í•˜ë˜ ì¢…ëª© ì½”ë“œ ëª©ë¡
    - day_df: ìƒˆë¡œ ìˆ˜ì§‘í•œ í•˜ë£¨ì¹˜ ë°ì´í„°
    """
    date_str = date_obj.strftime("%Y-%m-%d")
    new_codes = set(day_df["Code"].astype(str).str.zfill(6).unique())
    base_set = set(str(c).zfill(6) for c in baseline_codes)

    missing = sorted(base_set - new_codes)
    extra = sorted(new_codes - base_set)

    if missing:
        log(f"    âš  [WARN] {date_str}: ê¸°ì¡´ ì¢…ëª© ì¤‘ {len(missing)}ê°œ ëˆ„ë½")
        miss_path = os.path.join(LOG_DIR, f"missing_codes_{date_str}.txt")
        with open(miss_path, "w", encoding="utf-8") as f:
            f.write("\n".join(missing))
        log(f"       â†’ ëˆ„ë½ ì½”ë“œ ëª©ë¡: {miss_path}")

    if extra:
        log(f"    â„¹ [INFO] {date_str}: ì‹ ê·œ ì¶”ì • ì¢…ëª© {len(extra)}ê°œ")
        extra_path = os.path.join(LOG_DIR, f"extra_codes_{date_str}.txt")
        with open(extra_path, "w", encoding="utf-8") as f:
            f.write("\n".join(extra))
        log(f"       â†’ ì‹ ê·œ ì½”ë“œ ëª©ë¡: {extra_path}")

    # NaN ì²´í¬
    na_mask = day_df[["Open", "High", "Low", "Close", "Volume"]].isna().any(axis=1)
    if na_mask.any():
        n_nan = int(na_mask.sum())
        log(f"    âš  [WARN] {date_str}: OHLCV ê²°ì¸¡ í–‰ {n_nan}ê°œ ë°œê²¬")
        nan_path = os.path.join(LOG_DIR, f"nan_rows_{date_str}.csv")
        day_df.loc[na_mask, ["Date", "Code", "Open", "High", "Low", "Close", "Volume"]].to_csv(
            nan_path, index=False, encoding="utf-8-sig"
        )
        log(f"       â†’ ìƒì„¸ ëª©ë¡: {nan_path}")


# ---------------------------------------------------------
# 7. ì „ì²´ RAW ìë™ ì—…ë°ì´íŠ¸ ë©”ì¸ ë¡œì§
# ---------------------------------------------------------

def auto_update_raw(target_end: Optional[str] = None) -> None:
    """
    RAW_MAIN ê¸°ì¤€ìœ¼ë¡œ ìµœì‹  ë‚ ì§œ ì´í›„ì˜ ë°ì´í„°ë¥¼ ìë™ ìˆ˜ì§‘/ë³‘í•©.

    1) RAW_MAINì—ì„œ ìµœì‹  ë‚ ì§œ ì½ìŒ
    2) ìµœì‹  ë‚ ì§œ + 1ì¼ ~ target_end(ë˜ëŠ” ì–´ì œ)ê¹Œì§€ ë‚ ì§œ ëª©ë¡ ìƒì„±
    3) ê° ë‚ ì§œë³„ë¡œ ëª¨ë“  ì¢…ëª©ì— ëŒ€í•´ í•˜ë£¨ì¹˜ ë°ì´í„° ìˆ˜ì§‘
    4) DAILY ìŠ¤ëƒ…ìƒ· ì €ì¥ + ê²€ì¦ + RAW_MAINì— ë³‘í•© & ë°±ì—…
    """
    start_ts = time.time()

    log("")
    log("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    log("â”‚         RAW PATCH AUTO UPDATER (V3)         â”‚")
    log("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    log(f"[PATH] RAW_MAIN : {RAW_MAIN}")
    log(f"[PATH] DAILY    : {DAILY_DIR}")
    log(f"[PATH] LOGS     : {LOG_DIR}")

    if not os.path.exists(RAW_MAIN):
        log(f"[FATAL] RAW_MAIN íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {RAW_MAIN}")
        return

    # 1. ê¸°ì¡´ RAW ë¡œë“œ ë° ìµœì‹  ë‚ ì§œ/ê¸°ì¤€ ì¢…ëª© ì„¸íŠ¸ í™•ë³´
    df_raw = pd.read_parquet(RAW_MAIN)
    if df_raw.empty:
        log("[FATAL] RAW_MAINì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. (í–‰ ìˆ˜ 0)")
        return

    df_raw["Date"] = pd.to_datetime(df_raw["Date"])
    latest_date = df_raw["Date"].max().date()
    log(f"[STEP 1] í˜„ì¬ RAW ìµœì‹  ë‚ ì§œ: {latest_date}")

    latest_slice = df_raw[df_raw["Date"].dt.date == latest_date].copy()
    baseline_codes = sorted(latest_slice["Code"].astype(str).str.zfill(6).unique())
    universe_codes = sorted(df_raw["Code"].astype(str).str.zfill(6).unique())
    log(f"         - ê¸°ì¤€ ì¢…ëª© ìˆ˜(ìµœì‹ ì¼ ê¸°ì¤€): {len(baseline_codes):,}ê°œ")
    log(f"         - ì „ì²´ ìœ ë‹ˆë²„ìŠ¤ ì¢…ëª© ìˆ˜   : {len(universe_codes):,}ê°œ")

    # 2. target_end ê²°ì •
    if target_end is None:
        today = date.today()
        target_end_date = today - timedelta(days=1)
    else:
        target_end_date = pd.to_datetime(target_end).date()

    if target_end_date <= latest_date:
        log(f"[INFO] ì´ë¯¸ ìµœì‹ ì…ë‹ˆë‹¤. (ëª©í‘œ: {target_end_date}, í˜„ì¬: {latest_date})")
        return

    fetch_dates = generate_missing_dates(latest_date, target_end_date)
    if not fetch_dates:
        log("[INFO] ìˆ˜ì§‘í•  ì‹ ê·œ ë‚ ì§œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    log(f"[STEP 2] ìˆ˜ì§‘ ëŒ€ìƒ ë‚ ì§œ: {fetch_dates[0]} ~ {fetch_dates[-1]} (ì´ {len(fetch_dates)}ì¼)")

    all_new_data: List[pd.DataFrame] = []

    # 3. ë‚ ì§œë³„ ìˆ˜ì§‘ ë£¨í”„
    for d_idx, date_obj in enumerate(fetch_dates, start=1):
        date_str = date_obj.strftime("%Y-%m-%d")
        log("")
        log(f"â”Œâ”€ [STEP 3-{d_idx}] {date_str} ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
        day_start_ts = time.time()

        day_rows: List[pd.DataFrame] = []
        n_success = 0

        for c_idx, code in enumerate(universe_codes, start=1):
            df_day, status = fetch_single_day_multi(code, date_obj)
            if status == "success" and df_day is not None and not df_day.empty:
                day_rows.append(df_day)
                n_success += 1

            if c_idx % 500 == 0:
                log(f"    ... ì§„í–‰ì¤‘: {c_idx}/{len(universe_codes)}ê°œ ì½”ë“œ ì²˜ë¦¬ ì™„ë£Œ")

        if n_success == 0:
            log(f"â”‚   {date_str}: ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ì–´ ê±´ë„ˆëœ€ (íœ´ì¥ì¼ ë˜ëŠ” ì „ì²´ ì‹¤íŒ¨) â”‚")
            log("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
            continue

        day_df = pd.concat(day_rows, ignore_index=True)
        day_df["Code"] = day_df["Code"].astype(str).str.zfill(6)
        day_df["Date"] = pd.to_datetime(day_df["Date"])
        day_df = day_df.sort_values(["Date", "Code"]).reset_index(drop=True)

        log(f"â”‚   {date_str}: {n_success:,}ê°œ ì¢…ëª© ìˆ˜ì§‘ ì™„ë£Œ, ì´ í–‰ ìˆ˜ {len(day_df):,} â”‚")

        # 3-1. ê²€ì¦(ëˆ„ë½/NaN) + ë¡œê·¸ ì €ì¥
        verify_day_against_baseline(date_obj, day_df, baseline_codes)

        # 3-2. DAILY ìŠ¤ëƒ…ìƒ· ì €ì¥
        daily_path = os.path.join(DAILY_DIR, f"{date_obj.strftime('%Y%m%d')}.parquet")
        day_df.to_parquet(daily_path)
        log(f"â”‚   DAILY ì €ì¥: {daily_path} â”‚")

        elapsed_day = time.time() - day_start_ts
        log(f"â””â”€ [END   3-{d_idx}] {date_str} ì²˜ë¦¬ ì™„ë£Œ ({elapsed_day:,.1f}ì´ˆ) â”€â”€â”€â”€â”€â”˜")

        all_new_data.append(day_df)

    if not all_new_data:
        log("[INFO] ìƒˆë¡œ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ì–´ RAW ë³‘í•©ì„ ìƒëµí•©ë‹ˆë‹¤.")
        return

    # 4. ë³‘í•© ì „ ë°±ì—…
    log("")
    log("[STEP 4] RAW_MAIN ë°±ì—… ë° ë³‘í•© ì‹œì‘")
    backup_with_tag(RAW_MAIN)

    new_block = pd.concat(all_new_data, ignore_index=True)
    new_block["Code"] = new_block["Code"].astype(str).str.zfill(6)
    new_block["Date"] = pd.to_datetime(new_block["Date"])

    merged = pd.concat([df_raw, new_block], ignore_index=True)
    merged = merged.dropna(subset=["Date", "Code"])
    merged["Code"] = merged["Code"].astype(str).str.zfill(6)
    merged["Date"] = pd.to_datetime(merged["Date"])
    merged = merged.drop_duplicates(subset=["Date", "Code"], keep="last")
    merged = merged.sort_values(["Date", "Code"]).reset_index(drop=True)

    merged.to_parquet(RAW_MAIN)

    total_elapsed = time.time() - start_ts
    log("")
    log("ğŸ‰ [ì™„ë£Œ] RAW íŒ¨ì¹˜ ë° ë³‘í•© ì™„ë£Œ")
    log(f"    - ìµœì‹  ë‚ ì§œ : {merged['Date'].max().date()}")
    log(f"    - ì´ í–‰ ìˆ˜   : {len(merged):,}")
    log(f"    - ì „ì²´ ì†Œìš”ì‹œê°„: {total_elapsed:,.1f}ì´ˆ")
    log("=============================================================")


if __name__ == "__main__":
    # ì˜ˆì‹œ:
    #   python raw_patch.py               â†’ RAW ìµœì‹  ë‚ ì§œ ê¸°ì¤€, ì–´ì œê¹Œì§€ ìë™ ì—…ë°ì´íŠ¸
    #   python raw_patch.py 2025-11-18    â†’ ì§€ì • ë‚ ì§œê¹Œì§€ ì—…ë°ì´íŠ¸
    if len(sys.argv) > 1:
        auto_update_raw(sys.argv[1])
    else:
        auto_update_raw()
