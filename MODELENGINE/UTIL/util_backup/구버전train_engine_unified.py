# ============================================================
# Unified HOJ Trainer (V33 - Input Window Patch)
#   - í†µí•© DB(HOJ_DB_V31.parquet) í•˜ë‚˜ë¡œ Real/Research ëª¨ë‘ ì²˜ë¦¬
#   - ë™ì  íƒ€ê²Ÿ ìƒì„± (Horizon ììœ  ì¡°ì ˆ)
#   - Input Window í•„í„°ë§ (ì„¤ì •ëœ ê¸°ê°„ë³´ë‹¤ ê¸´ ì§€í‘œ ìë™ ì œì™¸)
#   * ë³¸ íŒŒì¼ì€ ê¸°ëŠ¥ ë³€ê²½ ì—†ì´ ì£¼ì„/ë¡œê·¸/ì •ë ¬ë§Œ ì •ë¦¬í•œ í´ë¦° ë²„ì „ì…ë‹ˆë‹¤.
# ============================================================

import os
import sys
import pickle
import argparse
import re
import glob  # [ì¶”ê°€] íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ìš©
from datetime import datetime, timedelta

import numpy as np
import pandas as pd
import lightgbm as lgb

# ------------------------------------------------------------
# 1. í”„ë¡œì íŠ¸ í™˜ê²½ ì„¤ì • (ê¸°ì¡´ ìœ í‹¸ ì—°ê²°)
# ------------------------------------------------------------
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)   # MODELENGINE
root_dir = os.path.dirname(parent_dir)      # Root
sys.path.append(root_dir)

try:
    from MODELENGINE.UTIL.config_paths import get_path
    from MODELENGINE.UTIL.version_utils import backup_existing_file
except ImportError:
    # UTIL í´ë” ë‚´ë¶€ì—ì„œ ì‹¤í–‰ë  ê²½ìš° ëŒ€ë¹„
    sys.path.append(parent_dir)
    from UTIL.config_paths import get_path
    from UTIL.version_utils import backup_existing_file

# ìµœì‹  ë‚ ì§œ íƒœê·¸ê°€ ë¶™ì€ DB íŒŒì¼ ìë™ íƒìƒ‰ (ê¸°ì¡´ ë™ì‘ ìœ ì§€)
from MODELENGINE.UTIL.version_utils import find_latest_file


# ------------------------------------------------------------
# 2. í•µì‹¬ í•¨ìˆ˜ ì •ì˜ (ë¡œì§ ë³€ê²½ ì—†ìŒ)
# ------------------------------------------------------------
def get_db_path(version: str = "V31") -> str:
    """
    HOJ_DB ë””ë ‰í† ë¦¬ì—ì„œ version íƒœê·¸ê°€ ë¶™ì€ ìµœì‹  parquet íŒŒì¼ì„ ìš°ì„  íƒìƒ‰.
    ì—†ìœ¼ë©´ ê¸°ë³¸ íŒŒì¼ëª…(HOJ_DB_{version}.parquet) ê²½ë¡œë¥¼ ë°˜í™˜.
    """
    base_dir = get_path("HOJ_DB")
    # REAL/RESEARCH í•˜ìœ„ë¡œ ë‚´ë ¤ê°€ ìˆëŠ” ê²½ìš° ìƒìœ„ë¡œ ë³´ì •
    if "REAL" in base_dir or "RESEARCH" in base_dir:
        base_dir = os.path.dirname(base_dir)

    db_name = f"HOJ_DB_{version}.parquet"
    db_path = os.path.join(base_dir, db_name)

    latest_db = find_latest_file(base_dir, f"HOJ_DB_{version}")
    return latest_db if latest_db else db_path


def ensure_datetime(df: pd.DataFrame, col: str = "Date") -> pd.DataFrame:
    """Date ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³´ì •."""
    if not np.issubdtype(df[col].dtype, np.datetime64):
        df[col] = pd.to_datetime(df[col], errors="coerce")
    return df


def build_dynamic_target(df: pd.DataFrame, horizon: int) -> pd.DataFrame:
    """
    ìš”ì²­í•œ Horizon(ì˜ˆ: 5ì¼)ì— ë§ëŠ” ì •ë‹µì§€(Return_{h}d, Label_{h}d)ê°€ ì—†ìœ¼ë©´ ì¦‰ì‹œ ìƒì„±.
    """
    ret_col = f"Return_{horizon}d"
    lab_col = f"Label_{horizon}d"

    if ret_col in df.columns and lab_col in df.columns:
        return df

    print(f"[Target] '{ret_col}' ìƒì„± ì¤‘ (Horizon={horizon})...")
    if "Close" not in df.columns:
        raise KeyError("DBì— 'Close' ì»¬ëŸ¼ì´ ì—†ì–´ íƒ€ê²Ÿì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    df = df.sort_values(["Code", "Date"]).copy()
    df[ret_col] = df.groupby("Code")["Close"].shift(-horizon) / df["Close"] - 1.0
    df[lab_col] = (df[ret_col] > 0).astype(int)
    return df


def get_save_filename(
    mode: str,
    version: str,
    data_date: str,
    horizon: int,
    input_window: int,
    n_estimators: int,
    train_date: str,
) -> str:
    """
    íŒŒì¼ëª… ê·œì¹™: HOJ_ENGINE_{MODE}_{VER}_d{yyyymmdd}_h{H}_w{IW or Full}_n{N}_t{yyMMdd}.pkl
    (ì›ë³¸ ê·œì¹™ ìœ ì§€, ì£¼ì„ë§Œ ëª…í™•í™”)
    """
    iw_tag = f"w{input_window}" if input_window > 0 else "wFull"
    name = (
        f"HOJ_ENGINE_{mode.upper()}_{version}_"
        f"d{data_date}_"
        f"h{horizon}_"
        f"{iw_tag}_"
        f"n{n_estimators}_"
        f"t{train_date}"
        ".pkl"
    )
    return name


# ------------------------------------------------------------
# 3. ë©”ì¸ íŠ¸ë ˆì´ë‹ ë¡œì§ (ê¸°ëŠ¥/ë¡œì§ ë³€ê²½ ì—†ìŒ)
# ------------------------------------------------------------
def run_unified_training(
    mode: str = "research",
    horizon: int = 5,
    input_window: int = 60,
    valid_days: int = 365,
    n_estimators: int = 1000,
    version: str = "V31",
) -> None:
    mode = mode.lower()
    print("\n=== [HOJ Engine Factory V33] ì‹œì‘ =========================")
    print(f"[Config] Mode={mode.upper()} | Horizon={horizon}d | InputWindow={input_window}d | Valid={valid_days}d")

    # [A] í†µí•© DB ë¡œë“œ
    db_path = get_db_path(version)
    if not os.path.exists(db_path):
        raise FileNotFoundError(f"DB íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {db_path}")

    print(f"[Load] DB: {os.path.basename(db_path)}")
    df = pd.read_parquet(db_path)
    df = ensure_datetime(df)
    df = df.sort_values(["Date", "Code"]).reset_index(drop=True)

    # [B] ë°ì´í„° ì •ë³´
    min_date = df["Date"].min().date()
    max_date_obj = df["Date"].max()
    max_date = max_date_obj.date()
    data_date_tag = max_date.strftime("%y%m%d")
    print(f"[Info] ë°ì´í„° ê¸°ê°„: {min_date} ~ {max_date} | Rows={len(df):,}")

    # --------------------------------------------------------------------------
    # [ì¶”ê°€] ì¤‘ë³µ íŒŒì¼ í™•ì¸ (Skip Logic)
    # ë°ì´í„° ë‚ ì§œ(d), ëª¨ë“œ, ì˜µì…˜ì´ ëª¨ë‘ ê°™ìœ¼ë©´ ì´ë¯¸ ìƒì„±ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ê³  SKIP
    # --------------------------------------------------------------------------
    # íŒŒì¼ëª… íŒ¨í„´ ìƒì„± (tëŠ” *ë¡œ ì™€ì¼ë“œì¹´ë“œ ì²˜ë¦¬)
    check_filename = get_save_filename(
        mode=mode,
        version=version,
        data_date=data_date_tag,
        horizon=horizon,
        input_window=input_window,
        n_estimators=n_estimators,
        train_date="*"
    )
    save_dir_check = get_path("HOJ_ENGINE", mode.upper())
    search_pattern = os.path.join(save_dir_check, check_filename)
    
    # í•´ë‹¹ íŒ¨í„´ì˜ íŒŒì¼ì´ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ SKIP
    existing_files = glob.glob(search_pattern)
    if existing_files:
        print("=" * 60)
        print(f"ğŸ›‘ [SKIP] ë™ì¼ ì¡°ê±´ì˜ ì—”ì§„ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
        print(f"   ì¡°ê±´: Mode={mode.upper()}, Date={data_date_tag}, H={horizon}, W={input_window}, N={n_estimators}")
        print(f"   ë°œê²¬ëœ íŒŒì¼: {os.path.basename(existing_files[0])}")
        print("=" * 60)
        return  # í•¨ìˆ˜ ì¢…ë£Œ
    # --------------------------------------------------------------------------

    # [C] íƒ€ê²Ÿ(ì •ë‹µ) ì¤€ë¹„
    df = build_dynamic_target(df, horizon)
    ret_col = f"Return_{horizon}d"
    lab_col = f"Label_{horizon}d"

    # [D] í”¼ì²˜ ì„ ì • (ê¸°ë³¸ ì œì™¸ ëª©ë¡ ìœ ì§€)
    exclude_cols = [
        "Code", "Date", "Name", "Market",
        "Open", "High", "Low", "Close", "Volume", "Amount", "Marcap",
        "KOSPI_ì¢…ê°€", "KOSPI_ìˆ˜ìµë¥ ",
        ret_col, lab_col, f"Expected_{ret_col}",
    ]
    # ë‹¤ë¥¸ horizon ë¼ë²¨ë“¤ë„ í•™ìŠµì—ì„œ ë°°ì œ (ì›ë³¸ ë¡œì§ ìœ ì§€)
    exclude_cols += [c for c in df.columns if (c.startswith("Return_") or c.startswith("Label_"))]

    feature_cols = df.columns.difference(exclude_cols).tolist()
    feature_cols = df[feature_cols].select_dtypes(include=["number", "bool"]).columns.tolist()

    # [D-1] Input Windowì— ë”°ë¥¸ í”¼ì²˜ í•„í„°ë§ (ì›ë³¸ ë¡œì§/ë™ì‘ ë™ì¼)
    if input_window > 0:
        print(f"[Filter] InputWindow={input_window} ì ìš© ì¤‘ (ê¸°ê°„ì´ ë” ê¸´ ì§€í‘œ ì œì™¸)...")
        final_features = []
        dropped_features = []

        for col in feature_cols:
            nums = re.findall(r"\d+", col)  # ëì˜ ìˆ«ìë¥¼ ê¸°ê°„ìœ¼ë¡œ ê°„ì£¼
            if nums:
                period = int(nums[-1])
                if period > input_window:
                    dropped_features.append(col)
                    continue
            final_features.append(col)

        if dropped_features:
            print(f"         ì œì™¸({len(dropped_features)}): {dropped_features}")
        feature_cols = final_features

    if not feature_cols:
        raise ValueError("í•™ìŠµí•  í”¼ì²˜ê°€ ì—†ìŠµë‹ˆë‹¤. Input Window ì„¤ì • ë˜ëŠ” DB ì»¬ëŸ¼ì„ í™•ì¸í•˜ì„¸ìš”.")

    print(f"[Feat] ìµœì¢… í•™ìŠµ í”¼ì²˜ ìˆ˜: {len(feature_cols)}")

    # [E] ê²°ì¸¡ ì œê±° (ì„ íƒëœ í”¼ì²˜ì— ëŒ€í•´ì„œë§Œ dropna â†’ ì›ë³¸ ë™ì‘)
    mask = df[feature_cols].notnull().all(axis=1) & df[ret_col].notnull()
    df_train = df[mask].copy()
    print(f"[Data] NaN ì œê±° í›„ í•™ìŠµ ë°ì´í„°: {len(df_train):,} rows (From {df_train['Date'].min().date()})")

    # [F] ëª¨ë¸ íŒŒë¼ë¯¸í„° ë° í•™ìŠµ (ì›ë³¸ ë™ì¼)
    params_common = {
        "n_estimators": n_estimators,
        "learning_rate": 0.03,
        "num_leaves": 63,
        "feature_fraction": 0.9,
        "bagging_fraction": 0.9,
        "bagging_freq": 3,
        "n_jobs": -1,
        "verbose": -1,
        "random_state": 42,
    }

    metrics = {}
    if mode == "research":
        split_date = max_date_obj - timedelta(days=valid_days)
        print(f"[Split] Research ëª¨ë“œ ê²€ì¦ ë¶„ë¦¬: split={split_date.date()}")

        mask_tr = df_train["Date"] < split_date
        mask_va = df_train["Date"] >= split_date

        X_tr = df_train.loc[mask_tr, feature_cols]
        y_tr_reg = df_train.loc[mask_tr, ret_col]
        y_tr_cls = df_train.loc[mask_tr, lab_col]

        X_va = df_train.loc[mask_va, feature_cols]
        y_va_reg = df_train.loc[mask_va, ret_col]
        y_va_cls = df_train.loc[mask_va, lab_col]

        print(f"[Size] Train={len(X_tr):,} | Valid={len(X_va):,}")

        print("[Train] íšŒê·€(Reg) & ë¶„ë¥˜(Cls) í•™ìŠµ...")
        model_reg = lgb.LGBMRegressor(objective="regression", metric="rmse", **params_common)
        model_reg.fit(X_tr, y_tr_reg, eval_set=[(X_va, y_va_reg)], callbacks=[lgb.early_stopping(100, verbose=False)])

        model_cls = lgb.LGBMClassifier(objective="binary", metric="binary_logloss", **params_common)
        model_cls.fit(X_tr, y_tr_cls, eval_set=[(X_va, y_va_cls)], callbacks=[lgb.early_stopping(100, verbose=False)])

        rmse = np.sqrt(np.mean((model_reg.predict(X_va) - y_va_reg) ** 2))
        acc = np.mean(model_cls.predict(X_va) == y_va_cls)
        print(f"[Eval] RMSE={rmse:.5f} | ACC={acc:.2%}")
        metrics = {"rmse": rmse, "acc": acc}

    else:
        print("[Train] Real ëª¨ë“œ: ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµ...")
        X_tr = df_train[feature_cols]
        y_tr_reg = df_train[ret_col]
        y_tr_cls = df_train[lab_col]

        model_reg = lgb.LGBMRegressor(objective="regression", metric="rmse", **params_common)
        model_reg.fit(X_tr, y_tr_reg)

        model_cls = lgb.LGBMClassifier(objective="binary", metric="binary_logloss", **params_common)
        model_cls.fit(X_tr, y_tr_cls)

        metrics = {"note": "Real mode full train"}

    # [G] ì €ì¥ (ê·œì¹™/ë™ì‘ ë™ì¼)
    train_date_tag = datetime.now().strftime("%y%m%d")
    save_name = get_save_filename(
        mode=mode,
        version=version,
        data_date=data_date_tag,
        horizon=horizon,
        input_window=input_window,
        n_estimators=n_estimators,
        train_date=train_date_tag,
    )

    save_dir = get_path("HOJ_ENGINE", mode.upper())
    save_path = os.path.join(save_dir, save_name)

    payload = {
        "model_reg": model_reg,
        "model_cls": model_cls,
        "features": feature_cols,
        "meta": {
            "mode": mode,
            "version": version,
            "horizon": horizon,
            "input_window": input_window,
            "n_estimators": n_estimators,
            "data_date": str(max_date),
            "train_date": str(datetime.now().date()),
            "metrics": metrics,
        },
    }

    with open(save_path, "wb") as f:
        pickle.dump(payload, f)

    print(f"[Save] ì—”ì§„ ì €ì¥ ì™„ë£Œ: {save_name}")
    print("=== [HOJ Engine Factory V33] ì™„ë£Œ =========================\n")


# ------------------------------------------------------------
# 4. CLI ì‹¤í–‰ë¶€ (ìˆ˜ì •: Research -> Real ìˆœì°¨ ìë™ ì‹¤í–‰)
# ------------------------------------------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--mode", default="all", help="ê¸°ë³¸ê°’ all: Research ì‹¤í–‰ í›„ Real ìë™ ì‹¤í–‰")
    parser.add_argument("--horizon", type=int, default=5, help="ì˜ˆì¸¡í•  ë¯¸ë˜ ì¼ìˆ˜ (ì˜ˆ: 5)")
    parser.add_argument("--input_window", type=int, default=60, help="ì…ë ¥ ê´€ì°° ê¸°ê°„ (ì˜ˆ: 60, 0ì´ë©´ ì „ì²´)")
    parser.add_argument("--valid_days", type=int, default=365)
    parser.add_argument("--n_estimators", type=int, default=1000)
    parser.add_argument("--version", type=str, default="V31")

    args = parser.parse_args()

    try:
        # [ë³€ê²½] ì‚¬ìš©ìê°€ --mode real ì´ë¼ê³  ëª…ì‹œí•˜ì§€ ì•ŠëŠ” í•œ, ê¸°ë³¸ì ìœ¼ë¡œ Research -> Real ìˆœì„œë¡œ ë‘˜ ë‹¤ ì‹¤í–‰
        modes_to_run = []
        
        if args.mode.lower() == "all" or args.mode.lower() == "research":
            modes_to_run.append("research")
        
        if args.mode.lower() == "all" or args.mode.lower() == "real":
            modes_to_run.append("real")
            
        # ìˆœì°¨ ì‹¤í–‰ (Research ë¨¼ì €, ê·¸ ë‹¤ìŒ Real)
        for m in modes_to_run:
            print(f"\nğŸš€ [Pipeline] {m.upper()} ì—”ì§„ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ ì§„ì…...")
            run_unified_training(
                mode=m,
                horizon=args.horizon,
                input_window=args.input_window,
                valid_days=args.valid_days,
                n_estimators=args.n_estimators,
                version=args.version,
            )
            
    except Exception as e:
        print(f"\n[Error] {e}")