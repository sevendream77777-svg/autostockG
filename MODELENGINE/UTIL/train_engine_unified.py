# ============================================================
# train_engine_unified_V31_complete.py
#  - í†µí•© DB í•˜ë‚˜(HOJ_DB_V31.*)ë¡œ REAL/RESEARCH ëª¨ë‘ í•™ìŠµ
#  - Aì•ˆ: ì„ íƒ í”¼ì²˜ì˜ 'ìµœëŒ€ ê¸°ê°„'ë§Œí¼ ê° ì¢…ëª© ì•êµ¬ê°„ ì œê±°(ì´ˆê¸° ì˜¤ì—¼ 0%)
#  - Horizon Tail: ê° ì¢…ëª© ë’¤ì—ì„œ hì¼ ì œê±°(ë¯¸ë˜ì •ë³´ ëˆ„ìˆ˜ 0%)
#  - input_window: 0ì´ë©´ ì „ì²´ í”¼ì²˜ ì‚¬ìš©, >0ì´ë©´ ê¸°ê°„ ì´ˆê³¼ ì§€í‘œ ì œì™¸
#  - Close/ClosePrice ìë™ ì¸ì‹ìœ¼ë¡œ íƒ€ê²Ÿ ìƒì„±
#  - meta ì €ì¥: feature_hash, data_date, horizon, input_window, valid_days ë“±
#  - ì €ì¥ ê·œì¹™: MODELENGINE/HOJ_ENGINE/{REAL|RESEARCH}/HOJ_ENGINE_{MODE}_YYYYMMDD_w{input_window}.pkl
#  - [ì¶”ê°€] ì‹¤í–‰ ì‹œ Research -> Real ìˆœì°¨ ìë™ ì‹¤í–‰ ì§€ì›
# ============================================================

import os
import sys
import re
INT_PAT = re.compile(r"\d+")
import pickle
import argparse
from datetime import datetime, timedelta

import numpy as np
import pandas as pd
import lightgbm as lgb

# ------------------------------------------------------------
# 0) ê²½ë¡œ ìœ í‹¸ (í”„ë¡œì íŠ¸ í‘œì¤€ ê²½ë¡œ ìš°ì„  ì‹œë„ â†’ í´ë°±)
# ------------------------------------------------------------
current_dir = os.path.dirname(os.path.abspath(__file__))
modelengine_dir = os.path.dirname(current_dir)   # .../MODELENGINE/UTIL â†’ .../MODELENGINE
root_dir = os.path.dirname(modelengine_dir)      # í”„ë¡œì íŠ¸ ë£¨íŠ¸
sys.path.extend([root_dir, modelengine_dir])

def _try_import_paths():
    """
    í”„ë¡œì íŠ¸ í‘œì¤€ ìœ í‹¸ ìš°ì„  ì‚¬ìš©.
    (ì—†ìœ¼ë©´ í´ë°±: ê¸°ë³¸ í´ë” ë ˆì´ì•„ì›ƒ ì¶”ì •)
    """
    HOJ_DB, HOJ_ENGINE_REAL, HOJ_ENGINE_RESEARCH, OUTPUT = None, None, None, None
    try:
        from MODELENGINE.UTIL.config_paths import get_path
        from MODELENGINE.UTIL.version_utils import find_latest_file
        def _get(purpose, *args): return get_path(purpose, *args)
        _find = find_latest_file
    except Exception:
        # í´ë°±: ê¸°ë³¸ í´ë” ë ˆì´ì•„ì›ƒ ì¶”ì •
        def _get(purpose, *args):
            base = os.path.join(root_dir, "MODELENGINE")
            if purpose == "HOJ_DB":
                return os.path.join(base, "HOJ_DB")
            if purpose == "HOJ_ENGINE":
                return os.path.join(base, "HOJ_ENGINE")
            if purpose == "OUTPUT":
                return os.path.join(base, "OUTPUT")
            # í•˜ìœ„ REAL/RESEARCH
            return os.path.join(base, "HOJ_ENGINE", args[0]) if args else os.path.join(base, "HOJ_ENGINE")

        def _find(folder, prefix):
            if not os.path.isdir(folder):
                return None
            cand = [os.path.join(folder, f) for f in os.listdir(folder) if f.startswith(prefix)]
            return sorted(cand)[-1] if cand else None

    return _get, _find

get_path, find_latest_file = _try_import_paths()

def ensure_dir(path: str):
    os.makedirs(path, exist_ok=True)
    return path

# ------------------------------------------------------------
# ì¶”ê°€: í•œ ì¤„ ê°±ì‹ í˜• LightGBM ë¡œê·¸ ì½œë°±
# ------------------------------------------------------------
def single_line_logger(period=50):
    def _callback(env):
        if period > 0 and env.iteration % period == 0:
            # env.evaluation_result_list ì˜ˆ: [('valid', 'rmse', 0.0234, False)]
            if env.evaluation_result_list:
                name, metric, value, _ = env.evaluation_result_list[0]
                msg = f"[{env.iteration}] {name}-{metric}: {value:.6f}"
                print("\r" + msg, end="", flush=True)
            # ë§ˆì§€ë§‰ì´ë©´ ì¤„ë°”ê¿ˆ
            if env.iteration == env.end_iteration - 1:
                print()
    _callback.order = 10
    return _callback

# ------------------------------------------------------------
# 1) ë°ì´í„° ë¡œë“œ/í”¼ì²˜ ì„ íƒ
# ------------------------------------------------------------
NUM_DTYPES = ("int", "uint", "float", "double")

NON_FEATURE_CANDIDATES = {
    "Date","date",
    "Code","code",
    "Name","name",
    "Open","High","Low","Close","ClosePrice","Adj Close","AdjClose","Volume","Amount",
    "open","high","low","close","volume","amount",
    "KOSPI_Close", "KOSPI_Change" # [ì¶”ê°€] KOSPI ë‹¨ìˆœ ì»¬ëŸ¼ì€ í”¼ì²˜í›„ë³´ì—ì„œ ì œì™¸
}

def pick_close_column(df: pd.DataFrame) -> str:
    """Close/ClosePrice/Adj Close ìë™ ì„ íƒ."""
    for c in ["Close","ClosePrice","Adj Close","AdjClose","close"]:
        if c in df.columns:
            return c
    raise KeyError("Close/ClosePrice/Adj Close ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")


def load_latest_db(version: str = "V31") -> pd.DataFrame:
    """HOJ_DB ê²½ë¡œì—ì„œ ìµœì‹  V31 íŒŒì¼ì„ ì°¾ì•„ ë¡œë“œ."""
    base_dir = get_path("HOJ_DB")
    if "REAL" in base_dir or "RESEARCH" in base_dir:
        base_dir = os.path.dirname(base_dir)
    latest = find_latest_file(base_dir, f"HOJ_DB_{version}")
    if latest is None:
        # ê¸°ë³¸ ì´ë¦„ í´ë°±
        cand = os.path.join(base_dir, f"HOJ_DB_{version}.parquet")
        if not os.path.exists(cand):
            raise FileNotFoundError(
                f"HOJ_DB íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {base_dir} (prefix=HOJ_DB_{version})"
            )
        return pd.read_parquet(cand)

    return pd.read_parquet(latest)

def select_feature_columns(df):
    # [ìˆ˜ì •] í”¼ì²˜ ì œì™¸ ë¦¬ìŠ¤íŠ¸ ë³´ê°•
    drop_cols = [
        'Date','Code','Open','High','Low','Close','Volume',
        'Return_1d','Return_5d','Label_1d','Label_5d',
        'KOSPI_Close', 'KOSPI_Change' 
    ]
    feats = []
    for col in df.columns:
        if col in drop_cols:
            continue
        if str(df[col].dtype).startswith(("float","int")):
            feats.append(col)
    return feats

def feature_period(col: str) -> int:
    """
    í”¼ì²˜ëª…ì—ì„œ ìµœëŒ€ ìˆ«ìë¥¼ ê¸°ê°„ìœ¼ë¡œ ì¶”ì¶œ (ì˜ˆ: sma120 â†’ 120, macd_12_26 â†’ 26).
    ìˆ«ìê°€ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ê°„ì£¼.
    """
    m = [int(x) for x in INT_PAT.findall(col)]
    return max(m) if m else 0

def apply_A_mask(df: pd.DataFrame, features: list, input_window: int, close_col: str, horizon: int) -> pd.DataFrame:
    """
    Aì•ˆ: ì„ íƒ í”¼ì²˜ë“¤ì˜ 'ìµœëŒ€ ê¸°ê°„'ë§Œí¼ ê° ì¢…ëª© ì•êµ¬ê°„ ì œê±° + ê° ì¢…ëª© ë’¤ hì¼ ì œê±°.
    """
    # input_window>0ì´ë©´ í•´ë‹¹ ì°½ì„ ì´ˆê³¼í•˜ëŠ” ê¸°ê°„ì˜ ì§€í‘œ ì œì™¸
    if input_window and input_window > 0:
        feats = []
        for c in features:
            p = feature_period(c)
            if (p == 0) or (p <= input_window):
                feats.append(c)
        features = feats

    # ìµœëŒ€ ê¸°ê°„ ê³„ì‚°
    max_period = max([feature_period(c) for c in features] + [0])

    # ì¢…ëª©ë³„ ì•/ë’¤ ìë¥´ê¸°
    parts = []
    for code, g in df.groupby("Code", sort=False):
        g = g.sort_values("Date")
        # ì•ìª½ ì œê±°
        if max_period > 0:
            g = g.iloc[max_period:].copy()
        # ë’¤ìª½ Tail ì œê±°
        if horizon > 0:
            g = g.iloc[:-horizon] if len(g) > horizon else g.iloc[0:0]
        parts.append(g)
    df_m = pd.concat(parts, ignore_index=True) if parts else df.iloc[0:0].copy()

    # íƒ€ê²Ÿ ìƒì„± (ë¯¸ë˜ hì¼ ìˆ˜ìµë¥ )
    df_m["TargetRet"] = (
        df_m.groupby("Code")[close_col].shift(-horizon) / df_m[close_col] - 1.0
    )
    df_m["TargetUp"] = (df_m["TargetRet"] > 0).astype("int8")

    # í•™ìŠµì— ì‚¬ìš©í•  ì»¬ëŸ¼ë§Œ ë‚¨ê¸°ê³  ê²°ì¸¡ ì œê±°
    use_cols = ["Date","Code", close_col] + features + ["TargetRet","TargetUp"]
    df_m = df_m[use_cols].dropna(subset=features + ["TargetRet"])
    return df_m, features, max_period

# ------------------------------------------------------------
# 3) í•™ìŠµ/ê²€ì¦ ìŠ¤í”Œë¦¿ & ëª¨ë¸ í•™ìŠµ
# ------------------------------------------------------------
def split_train_valid(df: pd.DataFrame, valid_days: int) -> tuple:
    max_day = df["Date"].max().normalize()
    valid_start = max_day - timedelta(days=int(valid_days))
    is_valid = df["Date"] >= valid_start
    train = df.loc[~is_valid].copy()
    valid = df.loc[ is_valid].copy()
    return train, valid, valid_start.date(), max_day.date()

def train_models(df_m: pd.DataFrame, features: list, n_estimators: int = 1000):
    X_tr = df_m.loc[df_m["is_train"], features]
    y_reg_tr = df_m.loc[df_m["is_train"], "TargetRet"]
    y_cls_tr = df_m.loc[df_m["is_train"], "TargetUp"]

    X_va = df_m.loc[~df_m["is_train"], features]
    y_reg_va = df_m.loc[~df_m["is_train"], "TargetRet"]
    y_cls_va = df_m.loc[~df_m["is_train"], "TargetUp"]

    # LightGBM Regressor
    model_reg = lgb.LGBMRegressor(
        n_estimators=n_estimators,
        max_depth=-1,
        learning_rate=0.03,
        subsample=0.9,
        colsample_bytree=0.9,
        objective="regression",
        n_jobs=-1
    )
    if len(X_va) > 0:
        model_reg.fit(X_tr, y_reg_tr,
                      eval_set=[(X_va, y_reg_va)],
                      eval_metric="rmse",
                      callbacks=[single_line_logger(period=50)])
    else:
        model_reg.fit(X_tr, y_reg_tr)

    # LightGBM Classifier (ìƒìŠ¹í™•ë¥ )
    model_cls = lgb.LGBMClassifier(
        n_estimators=n_estimators,
        max_depth=-1,
        learning_rate=0.03,
        subsample=0.9,
        colsample_bytree=0.9,
        objective="binary",
        n_jobs=-1
    )
    if len(X_va) > 0:
        model_cls.fit(X_tr, y_cls_tr,
                      eval_set=[(X_va, y_cls_va)],
                      eval_metric="auc",
                      callbacks=[single_line_logger(period=50)])
    else:
        model_cls.fit(X_tr, y_cls_tr)

    return model_reg, model_cls

# ------------------------------------------------------------
# 4) ì €ì¥
# ------------------------------------------------------------
def _hash_list(lst: list) -> str:
    return str(abs(hash("|".join(map(str, lst)))))

def save_engine(payload: dict, mode: str):
    base = get_path("HOJ_ENGINE")
    # REAL/RESEARCH í•˜ìœ„ í´ë” ë³´ì¥
    if os.path.isfile(base):
        base = os.path.dirname(base)
    out_dir = os.path.join(base, mode.upper())
    ensure_dir(out_dir)

    tag = datetime.strptime(payload["meta"]["data_date"], "%Y-%m-%d").strftime("%y%m%d")
    # === Aì•ˆ íŒŒì¼ëª… ê·œì¹™ ì ìš© ===
    # HOJ_ENGINE_{MODE}_V31_h{horizon}_w{window}_n{n}_{YYMMDD}.pkl
    # [ìˆ˜ì •] íŒŒì¼ëª… ìƒì„± ë¡œì§ (f-string ì¤‘ê´„í˜¸ ì˜¤ë¥˜ ìˆ˜ì • ë° ë‚ ì§œ ìŠ¬ë¼ì´ì‹±)
    fname = (
        f"HOJ_ENGINE_{mode.upper()}_V31"
        f"_h{payload['meta']['horizon']}"
        f"_w{payload['meta']['input_window']}"
        f"_n{payload['meta']['n_estimators']}"
        f"_{tag}.pkl"
    )

    path = os.path.join(out_dir, fname)
    with open(path, "wb") as f:
        pickle.dump(payload, f)

    print(f"\nğŸ’¾ ì—”ì§„ ì €ì¥ ì™„ë£Œ: {path}")

# ------------------------------------------------------------
# 5) ë©”ì¸ ëŸ¬ë„ˆ
# ------------------------------------------------------------
def run_unified_training(
    mode: str = "research",
    horizon: int = 5,
    input_window: int = 60,
    valid_days: int = 365,
    n_estimators: int = 1000,
    version: str = "V31",
):
    """
    mode: "real" | "research"
    """
    assert mode in ("real","research")

    print(f"=== ğŸš€ Unified HOJ Trainer V31 ({mode.upper()}) ===")
    print(f"[CFG] mode={mode}  horizon={horizon}  input_window={input_window}  valid_days={valid_days}  n_estimators={n_estimators}")

    # 1) DB ë¡œë“œ
    df = load_latest_db(version)
    close_col = pick_close_column(df)
    
    # [ë³´ì™„] Date ì»¬ëŸ¼ íƒ€ì… ì•ˆì „ ë³€í™˜
    if not pd.api.types.is_datetime64_any_dtype(df["Date"]):
        df["Date"] = pd.to_datetime(df["Date"])
        
    max_date = df["Date"].max().date()
    print(f"[DATA] DB max(Date) = {max_date}  | rows={len(df):,}")

    # ============================================================
    # [ì¶”ê°€] ì…êµ¬ì»· SKIP: ì´ë¯¸ ë™ì¼í•œ ì„¤ì •ê³¼ ë°ì´í„° ë‚ ì§œì˜ ì—”ì§„ì´ ìˆìœ¼ë©´ SKIP
    # ============================================================
    base = get_path("HOJ_ENGINE")
    if os.path.isfile(base):
        base = os.path.dirname(base)
    out_dir = os.path.join(base, mode.upper())
    ensure_dir(out_dir)

    tag_chk = max_date.strftime("%y%m%d") # 251126
    # íŒŒì¼ëª… ìƒì„± ê·œì¹™ (save_engineê³¼ ë™ì¼)
    fname_chk = (
        f"HOJ_ENGINE_{mode.upper()}_V31"
        f"_h{horizon}"
        f"_w{input_window}"
        f"_n{n_estimators}"
        f"_{tag_chk}.pkl"
    )
    path_chk = os.path.join(out_dir, fname_chk)

    if os.path.exists(path_chk):
        print(f"\n[SKIP] ì´ë¯¸ ë™ì¼í•œ ì„¤ì •ê³¼ ë°ì´í„° ë‚ ì§œì˜ ì—”ì§„ì´ ì¡´ì¬í•©ë‹ˆë‹¤.")
        print(f"       íŒŒì¼ëª…: {fname_chk}")
        print("       (í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.)")
        return # <--- í•´ë‹¹ ëª¨ë“œëŠ” ì¢…ë£Œ (ë£¨í”„ê°€ ìˆìœ¼ë©´ ë‹¤ìŒ ëª¨ë“œë¡œ ë„˜ì–´ê°)
    # ============================================================

    # 2) í”¼ì²˜ ì„ íƒ
    features = select_feature_columns(df)
    if close_col in features:
        features = [c for c in features if c != close_col]
    print(f"[FEAT] í›„ë³´ í”¼ì²˜ ìˆ˜ = {len(features)}")

    # 3) Aì•ˆ ë§ˆìŠ¤í¬ + Horizon Tail + íƒ€ê²Ÿ
    df_m, features, max_period = apply_A_mask(df, features, input_window, close_col, horizon)
    mask_min = df_m["Date"].min().date() if len(df_m) else None
    mask_max = df_m["Date"].max().date() if len(df_m) else None
    print(f"[MASK] MaxPeriod={max_period}d  | After Mask rows={len(df_m):,}  | Date range: {mask_min} ~ {mask_max}")

    # 4) ê²€ì¦ ë¶„ë¦¬
    if mode == "research":
        tr, va, valid_start, valid_end = split_train_valid(df_m, valid_days)
        tr["is_train"] = True
        va["is_train"] = False
        data = pd.concat([tr, va], ignore_index=True)
        print(f"[SPLIT] Train rows={len(tr):,}  Valid rows={len(va):,}  (valid={valid_start}~{valid_end})")
    else:
        data = df_m.copy()
        data["is_train"] = True
        print(f"[SPLIT] REAL ëª¨ë“œ: ì „ì²´ {len(data):,}í–‰ í•™ìŠµ, ê²€ì¦ ë¶„í•  ì—†ìŒ")

    # 5) í•™ìŠµ
    model_reg, model_cls = train_models(data, features, n_estimators=n_estimators)
    print("[TRAIN] LightGBM reg/cls í•™ìŠµ ì™„ë£Œ")

    # 6) ë©”íƒ€/í˜ì´ë¡œë“œ
    meta = {
        "version": "V31",
        "data_date": str(max_date),
        "horizon": int(horizon),
        "input_window": int(input_window),
        "valid_days": int(valid_days),
        "feature_hash": _hash_list(features),
        "trained_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "close_col": close_col,
        "n_estimators": int(n_estimators),
    }
    payload = {
        "model_reg": model_reg,
        "model_cls": model_cls,
        "features": features,
        "meta": meta,
    }

    # 7) ì €ì¥
    save_engine(payload, mode)

    print("=== ğŸ Done. ===")

# ------------------------------------------------------------
# 6) CLI
# ------------------------------------------------------------
if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    # [ìˆ˜ì •] default='all' ë¡œ ë³€ê²½ (Research -> Real ìˆœì°¨ ì‹¤í–‰)
    ap.add_argument("--mode", default="all", choices=["real","research","all"])
    ap.add_argument("--horizon", type=int, default=5)
    ap.add_argument("--input_window", type=int, default=60)
    ap.add_argument("--valid_days", type=int, default=365)
    ap.add_argument("--n_estimators", type=int, default=1000)
    ap.add_argument("--version", default="V31")
    args = ap.parse_args()

    # ì‹¤í–‰í•  ëª¨ë“œ ë¦¬ìŠ¤íŠ¸ ê²°ì •
    if args.mode == "all":
        modes_to_run = ["research", "real"]
    else:
        modes_to_run = [args.mode]

    try:
        # [ìˆ˜ì •] ìˆœì°¨ ì‹¤í–‰ ë£¨í”„
        for m in modes_to_run:
            run_unified_training(
                mode=m,
                horizon=args.horizon,
                input_window=args.input_window,
                valid_days=args.valid_days,
                n_estimators=args.n_estimators,
                version=args.version,
            )
            print("-" * 60) # êµ¬ë¶„ì„ 

    except Exception as e:
        print(f"\nâŒ [Error] {e}")
